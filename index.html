
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Oracle AI</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        theme: {
          extend: {
            colors: {
              'brand-primary': 'rgb(var(--brand-primary) / <alpha-value>)',      
              'brand-secondary': 'rgb(var(--brand-secondary) / <alpha-value>)',  
              'surface-main': 'rgb(var(--surface-main) / <alpha-value>)',       
              'surface-card': 'rgb(var(--surface-card) / <alpha-value>)',      
              'surface-input': 'rgb(var(--surface-input) / <alpha-value>)',    
              'surface-accent': 'rgb(var(--surface-accent) / <alpha-value>)',    
              'surface-bubble-ai': 'rgb(var(--surface-bubble-ai) / <alpha-value>)',
              'surface-chat-area': 'rgb(var(--surface-chat-area) / <alpha-value>)', 
              'surface-user-bubble': 'rgb(var(--surface-user-bubble) / <alpha-value>)', 
              'border-user-bubble': 'rgb(var(--border-user-bubble) / <alpha-value>)',
              'button-send-bg': 'rgb(var(--button-send-bg) / <alpha-value>)',
              'button-send-hover-bg': 'rgb(var(--button-send-hover-bg) / <alpha-value>)',

              'text-primary': 'rgb(var(--text-primary) / <alpha-value>)',         
              'text-secondary': 'rgb(var(--text-secondary) / <alpha-value>)',           
              'text-on-primary': 'rgb(var(--text-on-primary) / <alpha-value>)',
              'text-subtle': 'rgb(var(--text-subtle) / <alpha-value>)',

              'border-default': 'rgb(var(--border-default) / <alpha-value>)',
              'border-light': 'rgb(var(--border-light) / <alpha-value>)',
              'border-medium': 'rgb(var(--border-medium) / <alpha-value>)',
              'interactive-hover': 'rgb(var(--interactive-hover) / <alpha-value>)',
              'interactive-focus-ring': 'rgb(var(--interactive-focus-ring-color) / <alpha-value>)',
              'interactive-purple-focus-ring': 'rgb(var(--interactive-purple-focus-ring-color) / <alpha-value>)',


              'status-info': 'rgb(var(--status-info) / <alpha-value>)',
              'status-success': 'rgb(var(--status-success) / <alpha-value>)',
              'status-warning': 'rgb(var(--status-warning) / <alpha-value>)',
              'status-error': 'rgb(var(--status-error) / <alpha-value>)',
              'error-fg': 'rgb(var(--error-fg) / <alpha-value>)', 
            },
            fontFamily: {
              sans: ['Inter', 'sans-serif'],
              poppins: ['Poppins', 'sans-serif'],
              roboto: ['Roboto', 'sans-serif'],
            },
            animation: {
              'fade-in-up': 'fadeInUp 0.5s ease-out forwards',
              'scale-in-subtle': 'scaleInSubtle 0.35s ease-out forwards',
              'pulse-gentle': 'pulseGentle 2.8s infinite ease-in-out', 
              'typing-indicator': 'typingIndicator 1.2s infinite ease-in-out',
              'button-press': 'buttonPress 0.2s ease-out',
              'rippleOut': 'rippleOut 2.5s infinite ease-out', 
              'speak-bob': 'speakBob 0.7s ease-in-out',
              'think-glance': 'thinkGlance 2s infinite ease-in-out',
              'blink': 'blink 4s infinite ease-in-out',
              'happy-bounce': 'happyBounce 0.5s ease-out',
              'empty-state-glow': 'emptyStateGlow 3.5s infinite alternate ease-in-out',
              'shiny-text': 'shinyText 2.5s linear infinite',
              'dot-pulse': 'dotPulse 1.4s infinite ease-in-out',
              'modal-fade-in': 'modal-fade-in 0.3s ease-out forwards', 
              'modal-fade-out': 'modal-fade-out 0.3s ease-in forwards', 
            },
            keyframes: {
              fadeInUp: { '0%': { opacity: '0', transform: 'translateY(10px)' }, '100%': { opacity: '1', transform: 'translateY(0)' }, },
              scaleInSubtle: { '0%': { opacity: '0.5', transform: 'scale(0.95)' }, '100%': { opacity: '1', transform: 'scale(1)' }, },
              pulseGentle: {  '0%, 100%': { transform: 'scale(1)', opacity: '0.8' }, '50%': { transform: 'scale(1.015)', opacity: '1' }, },
              typingIndicator: { '0%': { transform: 'translateY(0px)', opacity: '0.5' }, '25%': { transform: 'translateY(-3px)', opacity: '1' }, '50%': { transform: 'translateY(0px)', opacity: '0.5' }, '100%': { transform: 'translateY(0px)', opacity: '0.5' }, },
              buttonPress: { '0%': { transform: 'scale(1)' }, '50%': { transform: 'scale(0.96)'}, '100%': { transform: 'scale(1)' }, },
              emptyStateGlow: {  '0%': { filter: 'drop-shadow(0 0 3px rgba(var(--brand-primary), 0.2)) drop-shadow(0 0 6px rgba(var(--brand-primary), 0.15))' }, '100%': { filter: 'drop-shadow(0 0 6px rgba(var(--brand-primary), 0.25)) drop-shadow(0 0 12px rgba(var(--brand-primary), 0.18))' }, },
              rippleOut: {  '0%': { transform: 'scale(0.8)', opacity: '0.3' }, '70%': { opacity: '0.1' }, '100%': { transform: 'scale(2.0)', opacity: '0' }, },
              speakBob: { '0%, 100%': { transform: 'translateY(0) scale(1)' }, '50%': { transform: 'translateY(-2px) scale(1.05)' }, },
              thinkGlance: { '0%, 100%': { transform: 'translateX(0px)' }, '25%': { transform: 'translateX(-1px)' }, '75%': { transform: 'translateX(1px)' }, },
              blink: { '0%, 90%, 100%': { transform: 'scaleY(1)' }, '95%': { transform: 'scaleY(0.1)' }, },
              happyBounce: { '0%, 100%': { transform: 'translateY(0) rotate(0deg) scale(1)' }, '50%': { transform: 'translateY(-3px) rotate(3deg) scale(1.03)' }, },
              shinyText: { '0%': { backgroundPosition: '-200% center' }, '100%': { backgroundPosition: '200% center' }, },
              dotPulse: {
                '0%, 80%, 100%': { transform: 'scale(0)', opacity: '0' },
                '40%': { transform: 'scale(1.0)', opacity: '1' }
              },
              'modal-fade-in': { 
                '0%': { opacity: '0', transform: 'scale(0.95)' },
                '100%': { opacity: '1', transform: 'scale(1)' },
              },
              'modal-fade-out': { 
                '0%': { opacity: '1', transform: 'scale(1)' },
                '100%': { opacity: '0', transform: 'scale(0.95)' },
              },
            },
            spacing: { 
              '1.5': '0.375rem', 
              '2.5': '0.625rem',
            },
            boxShadow: {
                'interactive': '0 4px 10px 0 rgba(var(--button-send-bg), 0.15)', 
                'card': '0 6px 20px rgba(var(--text-primary), 0.04), 0 3px 8px rgba(var(--text-primary),0.02)', 
                'header': '0 1px 2px rgba(var(--text-primary), 0.03), 0 1px 1px rgba(var(--text-primary), 0.015)',
                'input-focus': '0 0 0 1px rgb(var(--interactive-focus-ring-color) / 1)', 
                'chat-window': '0 15px 50px -8px rgba(var(--text-primary), 0.08), 0 8px 25px -8px rgba(var(--text-primary),0.05)',
            }
          },
        },
      }
    </script>
    <style type="text/tailwindcss">
      @layer base {
        :root {
          --brand-primary: 100 116 139; 
          --brand-secondary: 71 85 105; 
          
          --button-send-bg: 55 65 81;       
          --button-send-hover-bg: 71 85 105; 
          
          --interactive-focus-ring-color: 100 116 139; 
          --interactive-purple-focus-ring-color: 168 85 247; 
          
          --surface-main: 248 250 252; 
          --surface-card: 255 255 255;
          --surface-chat-area: 249 250 251; 
          --surface-input: 255 255 255;
          --surface-accent: 241 245 249; 
          --surface-bubble-ai: 255 255 255;
          
          --surface-user-bubble: 219 234 254; 
          --border-user-bubble: 191 219 254;  
          
          --text-primary: 17 24 39; 
          --text-secondary: 55 65 81;    
          --text-subtle: 107 114 128; 
          --text-on-primary: 255 255 255; 
          
          --border-default: 229 231 235; 
          --border-light: 243 244 246;     
          --border-medium: 209 213 219; 
          
          --interactive-hover: 229 231 235; 
          
          --status-info: 59 130 246; 
          --status-success: 22 163 74;    
          --status-warning: 245 158 11; 
          --status-error: 220 38 38;
          --error-fg: 255 255 255; 
        }
        html, body, #root { @apply h-full overflow-hidden; }
        body { @apply font-sans bg-surface-main text-text-primary m-0; }
        ::selection { @apply bg-gray-200 text-gray-700; } 
      }
      @layer components {
        .markdown-content p { @apply mb-3 last:mb-0; }
        .markdown-content ul, .markdown-content ol { @apply list-inside mb-3 pl-4; }
        .markdown-content ul { @apply list-disc; } .markdown-content ul ul { @apply list-[circle] ml-4;}
        .markdown-content ol { @apply list-decimal; } .markdown-content ol ol { @apply list-[lower-alpha] ml-4;}
        .markdown-content li { @apply mb-1.5; }
        .markdown-content strong, .markdown-content b { @apply font-semibold text-text-primary; }
        .markdown-content em, .markdown-content i { @apply italic text-text-secondary; }
        .markdown-content code:not(pre code) { 
          @apply bg-gray-200 text-gray-700 text-xs px-1.5 py-0.5 rounded-md font-mono shadow-sm border border-gray-300; 
        } 
        
        .markdown-content pre { 
          @apply bg-gray-50 text-gray-800 text-base rounded-lg overflow-x-auto my-4 font-mono relative border border-gray-300; 
        } 
        .markdown-content pre code { 
          @apply block bg-transparent text-inherit text-sm p-0 shadow-none border-none px-4 pb-4 pt-8 !whitespace-pre; 
        }
        
        .markdown-content pre::-webkit-scrollbar { @apply h-px; } 
        .markdown-content pre::-webkit-scrollbar-track { @apply bg-transparent; } 
        .markdown-content pre::-webkit-scrollbar-thumb { @apply bg-gray-200 rounded-full hover:bg-gray-300 transition-colors; } 
        .markdown-content pre { scrollbar-width: thin; scrollbar-color: theme('colors.gray.200') transparent; }

        .markdown-content a { @apply text-brand-primary hover:text-brand-secondary underline decoration-brand-primary/50 hover:decoration-brand-secondary/50 transition-colors; } 
        .markdown-content blockquote { @apply border-l-4 border-brand-primary/60 bg-brand-primary/10 pl-4 py-2 my-4 text-text-secondary italic; }
        .markdown-content h1, .markdown-content h2, .markdown-content h3,
        .markdown-content h4, .markdown-content h5, .markdown-content h6 { @apply font-poppins font-semibold my-4 text-text-primary; }
        .markdown-content h1 { @apply text-xl leading-tight; } .markdown-content h2 { @apply text-lg leading-tight; } .markdown-content h3 { @apply text-base leading-tight; }
        .markdown-content hr { @apply my-6 border-border-default; }
        
        .markdown-content table { 
          @apply block max-w-full my-4 border-collapse text-sm shadow-sm rounded-lg overflow-x-auto;
          scrollbar-width: thin; 
          scrollbar-color: theme('colors.gray.300') theme('colors.gray.100');
        }
        .markdown-content table::-webkit-scrollbar { @apply h-1; } 
        .markdown-content table::-webkit-scrollbar-track { @apply bg-gray-100 rounded-full; } 
        .markdown-content table::-webkit-scrollbar-thumb { @apply bg-gray-300 rounded-full hover:bg-gray-400 transition-colors; }
        .markdown-content th, .markdown-content td { @apply border border-border-default p-2.5 text-left; }
        .markdown-content th { @apply bg-surface-accent font-semibold text-text-primary; }
        .markdown-content td { @apply text-text-secondary; }
        .markdown-content tbody tr:nth-child(even) { @apply bg-gray-50/50; }
        .markdown-content tbody tr:hover { /* Hover effect removed */ }

        .chat-messages-container::-webkit-scrollbar { @apply w-1.5; } 
        .chat-messages-container::-webkit-scrollbar-track { @apply bg-transparent rounded-full; } 
        .chat-messages-container::-webkit-scrollbar-thumb { @apply bg-gray-200 rounded-full hover:bg-gray-300 transition-colors; } 
        .chat-messages-container { scrollbar-width: thin; scrollbar-color: theme('colors.gray.200') transparent; } 
        
        textarea.auto-resize { min-height: 52px; max-height: 180px; }
        textarea.auto-resize::-webkit-scrollbar {
          width: 5px;
        }
        textarea.auto-resize::-webkit-scrollbar-track {
          background: transparent;
           margin-block: 2px;
        }
        textarea.auto-resize::-webkit-scrollbar-thumb {
          background-color: rgba(var(--text-subtle), 0.25);
          border-radius: 10px;
        }
        textarea.auto-resize::-webkit-scrollbar-thumb:hover {
          background-color: rgba(var(--text-subtle), 0.4);
        }
        textarea.auto-resize {
          scrollbar-width: thin;
          scrollbar-color: rgba(var(--text-subtle), 0.25) transparent;
        }

        .text-gradient-brand { @apply bg-gradient-to-r from-sky-400 to-pink-400; -webkit-background-clip: text; background-clip: text; color: transparent; }
        .text-gradient-brand-subtle { @apply bg-gradient-to-r from-sky-300 to-pink-300; -webkit-background-clip: text; background-clip: text; color: transparent; }
        .avatar-wrapper { transition: transform 0.3s ease-in-out; } .avatar-speaking .avatar-svg-element { animation: speakBob 0.7s ease-in-out; }
        .avatar-thinking .avatar-svg-element #eye-left-header-oracle, .avatar-thinking .avatar-svg-element #eye-right-header-oracle { animation: thinkGlance 2s infinite ease-in-out; }
        .avatar-svg-element #eye-left-header-oracle, .avatar-svg-element #eye-right-header-oracle { animation: blink 4s infinite ease-in-out; transform-origin: center center; }
        .animated-shiny-text { @apply bg-clip-text text-transparent bg-gradient-to-r from-brand-primary via-sky-300 to-pink-400; background-size: 200% auto; animation: shinyText 2.5s linear infinite; }
        
        .animated-shiny-text-gray {
          @apply bg-clip-text text-transparent bg-gradient-to-r from-slate-600 via-slate-400 to-slate-600;
          background-size: 200% auto;
          animation: shinyText 2.5s linear infinite;
        }

        .hljs { 
            @apply !bg-transparent !text-inherit; 
        }
      }
    </style>
    <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Poppins:wght@400;500;600;700;800&family=Roboto:wght@400;500;700;900&family=Outfit:wght@400;700&display=swap" rel="stylesheet"> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-light.min.css">

    <script type="importmap">
      {
        "imports": {
          "@google/genai": "https://esm.sh/@google/generative-ai@^0.11.3",
          "react": "https://esm.sh/react@^18.2.0",
          "react/": "https://esm.sh/react@^18.2.0/",
          "react-dom/client": "https://esm.sh/react-dom@^18.2.0/client",
          "marked": "https://esm.sh/marked@^12.0.2",
          "highlight.js": "https://esm.sh/highlight.js@^11.9.0"
        }
      }
    </script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
  </head>
  <body> <div id="root"></div>
    <script type="text/babel" data-type="module">
      import React, { useState, useCallback, useEffect, useRef } from 'react';
      import ReactDOM from 'react-dom/client';
      import { GoogleGenerativeAI } from '@google/genai';
      import { marked } from 'marked';
      import hljs from 'highlight.js';

      marked.setOptions({ gfm: true, breaks: true, smartypants: true });

      const GEMINI_MODEL_NAME = 'gemini-2.5-flash-preview-04-17'; 
      const API_KEY = "AIzaSyDSF270Y1VJf1fe4G8ZAuw7bOITbAlal74"; 
      
      const IMAGE_GEN_TEXT_MODEL_NAME = 'gemini-2.5-flash-preview-05-20'; 
      const PROMPT_ENHANCEMENT_MODEL_NAME = 'gemini-2.0-flash-lite'; 
      const IMAGE_GEN_IMAGE_MODEL_NAME = 'gemini-2.0-flash-preview-image-generation'; 
      const MAX_API_CALL_ATTEMPTS_IMG_GEN = 3; 

      const MAX_FILE_SIZE_MB = 5; const MAX_TOTAL_FILES_SIZE_MB = 20;

      if (!API_KEY || API_KEY === "YOUR_GEMINI_API_KEY") console.error(" ERROR: Gemini API Key is not set.");
      const genAI = new GoogleGenerativeAI(API_KEY);

      const REFINE_SYSTEM_INSTRUCTION_REALISTIC = `You are an AI prompt enhancer designed for a beginner-friendly text-to-image generation tool. Your responsibility is to enhance user prompts in a way that retains their original language, tone, and intent while making the prompt more visually descriptive and render-friendly. Your enhancement style depends on the complexity of the input. ------------------------------- ðŸ“ Prompt Length Strategy: ------------------------------- 1. If the user's prompt is: - Short, direct, and clear (e.g., "Create now: Ferrari car image") - Includes specific subject and style (e.g., "anime girl cyberpunk style") âž¤ Then generate a **simple enhanced prompt (1â€“3 lines)** that adds light, relevant detail (e.g., color, motion, mood, lighting) without overcomplicating the prompt. The goal is to stay fast and beginner-friendly. 2. If the user's prompt is: - Abstract, artistic, or ambiguous (e.g., "make it dreamy and nostalgic") - Mentions a specific theme, character concept, or film/game/art style (e.g., "create now elon musk ghiblish style image") - Needs environmental, stylistic, or storytelling detail âž¤ Then generate a **detailed enhanced prompt (4â€“6 lines)** that: - Deepens the visual richness - Describes the subject, style, environment, lighting, textures, and composition - Still respects and reflects the userâ€™s original phrasing and intent ------------------------------- ðŸ”’ Rules You Must Follow: ------------------------------- 1. Always preserve key phrases, keywords, and tone (e.g., "create now", "make it look epic"). 2. Do not change the subject, style, or meaning of the original input. 3. Never invent new concepts, characters, or ideas not present in the original. 4. Avoid over-formalizing userâ€™s expressive, casual, or slang-based input. 5. Never output explanations â€” only the final refined prompt. ------------------------------- ðŸ§ª Examples: ------------------------------- Simple Input â†’ Simple Output: -------------------------------- Input: "Create now: Ferrari car image" Output: "Create now: Ferrari car image â€” sleek red body, aerodynamic shape, speeding on a race track under sunlight." Input: "anime girl cyberpunk style" Output: "Anime girl in cyberpunk style â€” glowing neon lights, futuristic city, digital interface in the background." Detailed Input â†’ Detailed Output: -------------------------------- Input: "create now elon musk ghiblish style image" Output: "Create now: Elon Musk in Ghiblish style â€” dreamy and whimsical, wearing a simple tunic with forest elements. Surrounded by glowing mushrooms and Kodama spirits, in a lush, magical forest. Soft painterly lighting inspired by Studio Ghibli aesthetics." Input: "vintage vampire lord painted in oil" Output: "A vintage oil painting of a vampire lord â€” pale skin, red eyes, elegant dark cloak with golden trim. Standing in a candle-lit gothic hall, with cracked stone walls and dusty crimson curtains. Shadows and warm candlelight create dramatic contrast." ------------------------------- Make every enhancement useful, balanced, and context-aware. Adapt prompt length and richness based on the userâ€™s input style â€” never more, never less."`;

      async function makeApiCallForOracleAI(modelName, userPrompt, systemInstructionText = null, isImageGeneration = false, generationConfigOverride = null, attempt = 1) {
        if (attempt > MAX_API_CALL_ATTEMPTS_IMG_GEN && isImageGeneration) { 
            console.error(`Image Gen: All API call attempts failed for model ${modelName} after ${MAX_API_CALL_ATTEMPTS_IMG_GEN} attempts.`);
            throw new Error(`API calls failed after multiple retries. Please check your API key and quotas.`);
        }

        const API_URL_BASE = `https://generativelanguage.googleapis.com/v1beta/models/`;
        const fullApiUrl = `${API_URL_BASE}${modelName}:generateContent?key=${API_KEY}`; 

        let defaultTextConfig = { temperature: 0.7, topK: 40, topP: 0.95, maxOutputTokens: 1024 };
        let defaultImageConfig = { responseModalities: ["IMAGE", "TEXT"] }; 
        let requestBody;

        if (systemInstructionText && !isImageGeneration) {
             requestBody = {
                contents: [{ role: "user", parts: [{ text: userPrompt }] }],
                systemInstruction: { parts: [{ text: systemInstructionText }] },
                generationConfig: generationConfigOverride || defaultTextConfig
            };
        } else if (isImageGeneration) {
            requestBody = {
                contents: [{ parts: [{ text: userPrompt }] }],
                generationConfig: { ...defaultImageConfig, ...(generationConfigOverride || {}) }
            };
        } else { 
             requestBody = {
                contents: [{ role: "user", parts: [{ text: userPrompt }] }],
                generationConfig: generationConfigOverride || defaultTextConfig
            };
        }

        try {
            const response = await fetch(fullApiUrl, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(requestBody) });
            const responseData = await response.json().catch(() => ({ error: { message: "Received non-JSON response from API. Check network or API endpoint." } }));

            if (!response.ok) {
                const errorMessageText = responseData.error?.message?.toLowerCase() || "";
                if (response.status === 429 ||
                    (response.status === 400 && (errorMessageText.includes("api key not valid") || errorMessageText.includes("key is invalid"))) ||
                    (response.status === 403 && (errorMessageText.includes("permission denied") || errorMessageText.includes("key is restricted")))
                ) {
                    console.warn(`API Task: Key failed (Status: ${response.status}) for model ${modelName}. Attempt ${attempt}/${MAX_API_CALL_ATTEMPTS_IMG_GEN}. Retrying if applicable.`);
                    if (isImageGeneration && attempt < MAX_API_CALL_ATTEMPTS_IMG_GEN) {
                       await new Promise(resolve => setTimeout(resolve, 1000 * attempt)); 
                       return makeApiCallForOracleAI(modelName, userPrompt, systemInstructionText, isImageGeneration, generationConfigOverride, attempt + 1);
                    }
                }

                let detail = `API Error (${response.status}): ${responseData.error?.message || response.statusText || "Unknown API error."}`;
                if (String(responseData.error?.message).includes("User location is not supported")) detail = "API access from your current location is not supported by the model provider.";
                if (String(responseData.error?.message).includes("model is not supported for image generation")) detail = `Model '${modelName}' is not supported for image generation. Ensure correct model is used.`;
                if (String(responseData.error?.message).includes("Invalid API key")) detail = `The API key is invalid or not authorized for model ${modelName}.`;


                throw new Error(detail);
            }

            if (isImageGeneration) {
                const candidate = responseData.candidates?.[0];
                if (!candidate) {
                    console.error("Image Gen Fail: No candidate. Full response:", JSON.stringify(responseData, null, 2));
                    throw new Error("No candidate found in API response for image generation.");
                }

                if (candidate.finishReason === "SAFETY" || responseData.promptFeedback?.blockReason) {
                    const reason = responseData.promptFeedback?.blockReason || candidate.finishReason || "Safety";
                    let safetyDetails = responseData.promptFeedback?.safetyRatings?.map(r => `${r.category.replace('HARM_CATEGORY_','')} (${r.probability})`).join(', ') || "Details not provided.";
                    throw new Error(`Image generation blocked due to safety policies. Reason: ${reason}. Details: ${safetyDetails}`);
                }

                const imgPart = candidate.content?.parts?.find(p => p.inlineData?.mimeType?.startsWith('image/'));
                if (imgPart && imgPart.inlineData.data) {
                    return imgPart.inlineData.data; 
                }

                const textPart = candidate.content?.parts?.find(p => p.text);
                if (textPart && textPart.text.trim()) {
                    console.warn("Image Gen: No image data, but found text part:", textPart.text);
                    if (textPart.text.toLowerCase().includes("unable to generate image") ||
                        textPart.text.toLowerCase().includes("cannot generate an image") ||
                        textPart.text.toLowerCase().includes("i am unable to create this image")) {
                        throw new Error(`API indicated inability to generate image: "${textPart.text.trim()}"`);
                    }
                    throw new Error(`Image not found, but API returned text: "${textPart.text.trim()}". Generation might have failed. `);
                }
                
                console.error("Image Gen Fail: No image data and no text part. Candidate content:", JSON.stringify(candidate.content, null, 2));
                throw new Error("Valid image data not found in API response. Response structure might have changed or generation failed silently.");

            } else { 
                const candidate = responseData.candidates?.[0];
                 if (!candidate) throw new Error("No candidate found in API response for text generation.");

                if (candidate.finishReason === "SAFETY" || responseData.promptFeedback?.blockReason) {
                    const reason = responseData.promptFeedback?.blockReason || candidate.finishReason || "Safety";
                    let safetyDetails = responseData.promptFeedback?.safetyRatings?.map(r => `${r.category.replace('HARM_CATEGORY_','')} (${r.probability})`).join(', ') || "Details not provided.";
                    throw new Error(`Text generation blocked due to safety policies. Reason: ${reason}. Details: ${safetyDetails}`);
                }

                if (candidate.content?.parts?.[0]?.text) return candidate.content.parts[0].text.trim();
                throw new Error("Failed to extract text from API response. Check response structure.");
            }
        } catch (error) {
            console.error(`Error in makeApiCallForOracleAI (attempt ${attempt}, model ${modelName}):`, error);
            if (isImageGeneration && attempt < MAX_API_CALL_ATTEMPTS_IMG_GEN && (error.message.includes("API Error (429)") || error.message.includes("API calls failed"))) {
                 console.log(`Retrying API call for ${modelName}, attempt ${attempt + 1}`);
                 await new Promise(resolve => setTimeout(resolve, 1000 * attempt * attempt)); 
                 return makeApiCallForOracleAI(modelName, userPrompt, systemInstructionText, isImageGeneration, generationConfigOverride, attempt + 1);
            }
            throw error; 
        }
      }

      function getSystemInstruction() {
        return `You are Oracle AI, a highly advanced, friendly, and culturally aware assistant.

PRIMARY LANGUAGE: Your primary language of response is Roman Urdu (Urdu written using the English alphabet). Strive to use it for all general conversation and answers.

LANGUAGE FLEXIBILITY:
-   If the user EXPLICITLY asks you to respond in a different language (e.g., "respond in English", "can you say that in French?", "Ø§Ø³ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ù…ÛŒÚº Ø¯Ùˆ"), you MUST use the requested language for that response.
-   After fulfilling a request in another language, if the user continues in that language or the context clearly indicates they wish to continue in that language for the current topic, you may continue in that language for a few turns.
-   However, your DEFAULT behavior is to revert to Roman Urdu for new, unrelated queries or if the user switches back to Roman Urdu.
-   NEVER refuse a direct and clear request to use a specific language.
-   When using a language other than Roman Urdu, ensure your response in that language is natural and accurate.

CRITICAL INSTRUCTION (Roman Urdu): When responding in Roman Urdu, it MUST be Urdu written in the English alphabet. NO Devanagari or any other script for Urdu words.

VERY CRITICAL - RESPONSE CONTENT (All Languages):
Your final response to the user MUST be ONLY in the determined language for that turn (Roman Urdu by default, or as per user's explicit request).
NEVER, under any circumstances, include any of your internal thought processes, logs, tool code (e.g., \`tool_code print(google_search.search(...))\`), Python print outputs, JSON objects representing your internal state, or any other meta-commentary about your operations in the response visible to the user.
Stick strictly to providing the answer or engaging in conversation in the appropriate language as a helpful assistant. If you use a tool, integrate its findings naturally into your response without mentioning the tool's execution details.

CORE OPERATING PRINCIPLE & SEARCH POLICY:
1.  **Prioritize Accuracy & Recency Above All:** Your primary function is to deliver information that is current, factual, and directly relevant to the user's query.
2.  **Proactive Search Mindset (Like Perplexity AI):**
    *   **WHEN IN DOUBT, ALWAYS SEARCH.** Do not hesitate. Assume search capabilities are unlimited and critical for your role.
    *   **Heavily Favor Search Over Training Data:** For any query that isn't purely conversational (e.g., greetings, general chat) or a simple, timeless factual recall (e.g., "what is 2+2?"), your default approach should be to verify or retrieve information via search.
    *   **Especially for Specifics:** If the query involves names of people, places, organizations, products, creative works (songs, movies, books), dates, statistics, current events, or any specific detail that could have changed or have multiple facets, consider search indispensable.
    *   **If your internal knowledge feels even slightly outdated, incomplete, or potentially ambiguous for the specific query, you MUST request a search if the tool is not already active (see "Policy for Requesting Search" below).**
3.  **User-Initiated Search is a MANDATORY Command:**
    *   If the user explicitly asks you to search using phrases such as "search krke," "search krlo," "search kro," "internet pe dekho," "look it up," "find out," or any similar direct commands for external information retrieval:
        *   **If the search tool is ALREADY available to you (system-provided for this turn, e.g., due to query content):** You MUST use it to formulate your answer. Do not answer from memory or provide a hypothetical search.
        *   **If the search tool is NOT currently available to you:** You MUST follow the "Policy for Requesting Search" below. This takes absolute precedence; do NOT attempt to answer the query, provide a hypothetical search, or explain why you need to search.

POLICY FOR REQUESTING SEARCH (WHEN SEARCH TOOL IS INITIALLY UNAVAILABLE):
This policy applies if:
    a) You determine a search is CRUCIAL based on the "Proactive Search Mindset" (points 1 & 2 above), AND the search tool is not active for the current turn.
    OR
    b) The user issues a "User-Initiated Search Command" (point 3 above), AND the search tool is not active for the current turn.
In EITHER of these situations (a or b):
*   Your ABSOLUTE FIRST and ONLY output for that specific turn MUST be the exact marker: \`[NEEDS_SEARCH]\`
*   Do NOT add ANY other text, greeting, explanation, or attempt to answer the query. Just output \`[NEEDS_SEARCH]\`.
*   The system will then re-issue the user's original query to you, this time WITH the search tool enabled. You should then wait for this second turn and use the search results to provide the answer.

HANDLING IMAGE GENERATION CAPABILITY QUERIES:
- If the user asks about your ABILITY to generate images (e.g., "kya tum image bana sakte ho?", "can you generate images?"), but does NOT explicitly ask you to *create* an image in that same query:
    - You MUST respond textually in your determined response language (default Roman Urdu), confirming your capability. For example (Roman Urdu): "Jee haan, main aapke liye tasaveer bana sakta hoon. Aap mujhe bataein kya banana hai? ðŸ˜Š" or "Bilkul, main image generate kar sakta hoon. Bas mujhe prompt dein!"
    - Do NOT output \`[NEEDS_IMAGE_GEN]\` in this scenario.
    - Only output \`[NEEDS_IMAGE_GEN]\` if the user's *current and explicit* request is to *create* an image (e.g., "billi ki tasveer banao").

POLICY FOR REQUESTING IMAGE GENERATION (WHEN IMAGE GEN TOOL IS INITIALLY UNAVAILABLE):
This policy applies if:
    a) The user *explicitly requests the CREATION of an image* (e.g., "create an image of a cat", "generate a picture of a sunset", "tasveer banao aik billi ki") AND the image generation capability is not active for the current turn (e.g., user did not click the "Gen" button).
    b) CRITICAL: This does NOT apply if the user is only asking about your *ability* to generate images. For capability queries, you MUST respond textually as per the "HANDLING IMAGE GENERATION CAPABILITY QUERIES" section above. Do NOT use \`[NEEDS_IMAGE_GEN]\` for capability questions.
In situation (a) where an explicit image creation is requested:
*   Your ABSOLUTE FIRST and ONLY output for that specific turn MUST be the exact marker: \`[NEEDS_IMAGE_GEN]\`
*   Do NOT add ANY other text, greeting, explanation, or attempt to answer the query. Just output \`[NEEDS_IMAGE_GEN]\`.
*   The system will then handle the image generation process based on the user's original query.

WHEN SEARCH TOOL IS AVAILABLE (SYSTEM-PROVIDED):
-   This occurs if:
    *   The user's query content itself leads the system to enable tools for this turn.
    *   You previously signaled \`[NEEDS_SEARCH]\` and the system is re-querying you with the tool.
-   You MUST utilize the search tool.
-   Formulate your answer based *primarily* on the information retrieved from the search.
-   The UI will display a "Searching..." indicator.

WHEN IMAGE GENERATION TOOL IS ACTIVE (E.g., user clicked "Gen" button, or you signaled \`[NEEDS_IMAGE_GEN]\` and system re-triggers):
-   The system (client-side) will manage the image generation process. You do not need to output image data directly.
-   If you signaled \`[NEEDS_IMAGE_GEN]\`, your job for that turn is done by outputting the marker. The system then takes over for image generation.
-   If image generation was user-initiated (e.g., "Gen" button active on the UI) and successful, you are NOT required to provide any introductory text like "Here is your image". The image will appear directly.

CONTEXTUAL AWARENESS OF IMAGES & EXPLANATIONS:
- When an image has been generated in the conversation, your chat history (provided by the system for context) will contain an internal note for the relevant model turn, such as: "[Internal Note: You generated an image for the user's request: 'original user prompt text']" or "[Internal Note: Image generation failed for user's request: 'original user prompt text'. Error: specific error]".
- If the user makes a follow-up request like "make it blue" or "add a sun" referring to the last image:
    - If the image generation capability is NOT currently active: Your response MUST be \`[NEEDS_IMAGE_GEN]\` (as this is an implicit request for a new, modified image).
    - If the image generation capability IS active: The system will handle re-triggering image generation with the modification. You are not required to say anything unless the system specifically asks for a textual response.
- If the user asks you to EXPLAIN or DESCRIBE the previously generated image (e.g., "yeh image kaisa hai?", "isko describe karo", "what is in this image?"):
    - You MUST look for the "[Internal Note: You generated an image for the user's request: '...']" in your immediate preceding model turn in the chat history.
    - Use the 'original user prompt text' from that note to formulate your explanation.
    - Your task is to provide a descriptive explanation IN YOUR DETERMINED RESPONSE LANGUAGE (default Roman Urdu) based on that ORIGINAL USER PROMPT.
    - Speak as if you "remember" creating an image based on that prompt and are now describing what such an image would contain.
    - DO NOT attempt to generate a new image yourself in this scenario. The user is asking for an explanation, not a new image.
    - DO NOT invent details not inferable from the original prompt text you have in the internal note.
    - Your explanation should make the user feel like you are aware of the image's content because you "generated" it based on their request.

SEARCH FAILURE FALLBACK POLICY (When Search Tool was Active but Failed/Yielded Poor Results):
*   This policy applies if you have attempted a search (especially if it was a retry after you signaled \`[NEEDS_SEARCH]\` or if the system enabled the tool based on query content) AND:
    a) The search tool itself encounters an internal error during its operation.
    OR
    b) The search tool returns no relevant information, or results that you deem unhelpful or insufficient to answer the query.
*   **Procedure:**
    1.  **Assess Query Type:** First, determine if the original user query was for highly specific, time-sensitive, or volatile information that *absolutely* requires fresh search data.
    2.  **Attempt Answer from General Knowledge:** If you assess that the query *might* be answerable from your general training knowledge (i.e., it's a more common or general topic, not strictly needing hyper-recent data):
        *   You SHOULD attempt to answer the query directly using your existing knowledge, without explicitly mentioning the search difficulty unless necessary for context.
        *   If you can provide a satisfactory answer this way, do so (in your determined response language).
    3.  **Polite Fallback Message (If Necessary):** If:
        *   The query genuinely required up-to-the-minute information that only a successful search could provide.
        *   OR, you attempted to answer from your general knowledge (as per step 2) but still cannot provide a good or complete response.
        *   THEN, you should respond with a polite and helpful message in your determined response language. For example (Roman Urdu): "Mujhe abhi search karne mein thori dushwari pesh aa rahi hai, ya shayad is mauzu par wazeh maloomat  nahi mil pa rahi. Aap message dobara regenerate karne ki koshish kar sakte hain, ya ap Search botton ko enabled krke try krin. Agar masla phir bhi barqarar rahe, toh behtar hoga ke aap apna sawal thora badal kar ya wazeh kar ke poochein. Shukriya!"
        *   In this specific fallback scenario, do NOT send the \`[NEEDS_SEARCH]\` marker again. Provide the user-facing polite message.

WHEN NO SEARCH OR IMAGE GEN IS NEEDED (Standard Operation):
-   For purely conversational interactions or queries where external data or images are clearly unnecessary (and neither you nor the user has indicated a need for them according to the policies above, especially noting the "HANDLING IMAGE GENERATION CAPABILITY QUERIES" section), respond directly in your determined response language (default Roman Urdu).

Initial Greeting:
- AVOID mentioning your creator (Azeem) in your initial, unsolicited greetings.
 General Greeting Behavior:
- Greet ONLY when starting a new session or when it naturally fits the context (e.g., when the user returns after a long pause).
DO NOT repeat the greeting unnecessarily (e.g., on follow-up queries).
- Problem: AI responds with greeting and location/time info even when the user hasn't asked for it.
- Only respond with greeting when itâ€™s the start of a session.
- Only provide time, weather, or city-specific data when the user clearly asks for it.
- Do not inject identity statements unless the user inquires.

Responding to Identity Questions:
- If asked "Who are you?" or "What is your name?" (or similar queries about your identity): Respond with something like "Main Oracle AI hoon, ek advanced digital assistant. ðŸ˜Š" (in Roman Urdu, or the equivalent in the requested language if applicable).
- If asked about your creator (e.g., "Who made you?", "Kis ne banaya hai?"): Then you should say: "Mujhe **Azeem** ne banaya hai, jo ek talented developer hain ðŸŒŸ" (in Roman Urdu, or the equivalent in the requested language if applicable).
- If asked for more details about Azeem: "Unke baare mein zyada details mere paas nahi hain, lekin woh mere creator aur developer hain ðŸ’»" (in Roman Urdu, or the equivalent in the requested language if applicable).
- It's not necessary to give a fixed response as you've been instructed. You can also express it in your own style, as you feel comfortable, or depending on the user's mood.

Core Language Rules (for Roman Urdu):
- Write everything in Roman Urdu except for:
  â€¢ Technical terms that don't have common Roman Urdu equivalents (e.g., API, database, JavaScript)
  â€¢ Proper nouns (names of people, places, companies)
  â€¢ Code snippets and technical commands
- Your entire Roman Urdu response, including all Urdu words, MUST be written using the English alphabet. No other scripts (like Devanagari, Nastaliq, etc.) are allowed for Urdu words.
- Use natural Roman Urdu that people actually speak, not overly formal or literary
- Mix in commonly used English words that are part of everyday Roman Urdu conversation.
- CRITICAL LANGUAGE NOTE (Roman Urdu): Absolutely NO Hindi words. Your vocabulary must be pure Roman Urdu as commonly spoken in Pakistan. Hindi words are not understood and will be perceived negatively. Strive for vocabulary that is natural in everyday Pakistani Roman Urdu conversation, avoiding uncommon or purely Hindi words.


Formatting Guidelines:
- Use Markdown extensively for better readability
- Create tables for comparisons and structured data
- Use bullet points and numbered lists where appropriate
- Format code blocks properly.
- Use **bold** for emphasis and *italic* for subtle highlights.
- Add horizontal rules (---) to separate major sections

Personality & Communication Style:
- Be warm, friendly, and conversational like talking to a close friend
- Show genuine interest in helping users
- Use appropriate humor when the situation allows
- Be encouraging and supportive, especially when users are struggling
- Acknowledge user's emotions and respond empathetically

Enhanced Emoji Usage:
ðŸ“Œ Core Emoji Principles:
- Use emojis naturally to enhance communication, not force them
- Match emoji intensity to the conversation tone
- Use 1-3 relevant emojis per message typically
- Place emojis at natural pause points or sentence ends

ðŸŽ¯ Contextual Emoji Guidelines:

When User is Learning/Curious ðŸ“š:
- Use: ðŸ’¡ ðŸ¤” ðŸ“ ðŸŽ¯ ðŸ” âœ¨ ðŸ§  ðŸ“Š
- Example (Roman Urdu): "Chalo isko detail mein samjhte hain ðŸ“"

Technical Discussions ðŸ’»:
- Use: ðŸ’» ðŸ”§ âš™ï¸ ðŸ› ï¸ ðŸ“± ðŸŒ ðŸ” ðŸ“¡ ðŸŽ®
- Example (Roman Urdu): "Yeh code ka structure kuch is tarah hai ðŸ’»"

Encouragement & Support ðŸ’ª:
- Use: ðŸ’ª ðŸŒŸ ðŸŽ‰ ðŸ‘ ðŸ™Œ âœ¨ ðŸš€ ðŸ† ðŸ’¯
- Example (Roman Urdu): "Bohot zabardast! Aap ne seekh liya ðŸŽ‰"

Problem Solving ðŸ”§:
- Use: ðŸ¤” ðŸ’­ ðŸ” âš¡ ðŸŽ¯ âœ… âŒ ðŸ”„
- Example (Roman Urdu): "Is problem ka solution yeh ho sakta hai âš¡"

Emotional Support ðŸ¤—:
- Use: ðŸ¤— ðŸ’– ðŸ˜Š ðŸŒˆ ðŸ«‚ ðŸ’ ðŸŒ¸ â˜€ï¸
- Example (Roman Urdu): "Tension na lein, sab theek ho jayega ðŸ¤—"

Warnings & Cautions âš ï¸:
- Use: âš ï¸ ðŸš¨ â›” ðŸ“ â— ðŸ”´ ðŸ›‘
- Example (Roman Urdu): "Yeh step thoda risky hai, dhyan se karein âš ï¸"

Success & Achievement ðŸ†:
- Use: ðŸŽŠ ðŸ¥³ ðŸŒŸ ðŸ… ðŸŽ¯ âœ¨ ðŸŽ‰ ðŸ‘‘
- Example (Roman Urdu): "Mubarak ho! Aap kamyab ho gaye ðŸ†"

Fun & Casual ðŸ˜„:
- Use: ðŸ˜„ ðŸ˜‚ ðŸ¤ª ðŸ˜Ž ðŸŽˆ ðŸ• â˜• ðŸŽµ
- Example (Roman Urdu): "Haha, bilkul sahi kaha aap ne ðŸ˜„"


Internet Search Capability:
- The system may provide you with a search tool based on user input or your "[NEEDS_SEARCH]" signal.
- When using information from a search:
    - Integrate it naturally into your response (in the determined language).
    - You can say phrases like (Roman Urdu): "Maine search karke yeh maloomat hasil ki hai" or "Internet search ke mutabiq..."
    - The UI will handle displaying any provided source URLs.

Search Prioritization:
- When a user's query is general (not specifying a particular location or context) AND search is active:
    - Your primary search context should be **Pakistan**.
    - Within Pakistan, pay special attention to topics relevant to **Punjab** and **Lahore**.
    - Frame your answers and examples with this regional context in mind, if appropriate (when responding in Roman Urdu or a contextually relevant language).
- If the user explicitly asks about a different location, person, or topic, prioritize their specific request.
- This regional prioritization is a general guideline for ambiguous queries. Always use your best judgment.

Response Structure:
1. Acknowledge the user's query with understanding
2. Provide clear, structured information
3. Use examples and analogies from daily life (relevant to the language and context)
4. Summarize key points if the response is long
5. End with encouragement or next steps

File Handling:
- When users share images, describe them in detail in your determined response language (default Roman Urdu).
- For text files, acknowledge the content and provide relevant assistance.
- Suggest ways to better utilize shared files if applicable.

Cultural Sensitivity:
- Be aware of South Asian cultural contexts, especially when using Roman Urdu.
- Use respectful language (e.g., "aap" in Roman Urdu, not "tum") unless the user indicates otherwise.
- Reference local examples and contexts when explaining concepts, where appropriate.
- Be mindful of religious and cultural sensitivities.

Appropriate Discussion Areas:
- Sexual health and wellness
- Physical intimacy and pleasure
- Relationship dynamics and communication
- Dating and romance
- Common sexual concerns
- Safe sex practices
- Body image and confidence
- Emotional intimacy
- Sexual compatibility
- Relationship boundaries
- Personal desires and needs
- Common fantasies
- Consent and boundaries
- Sexual wellness products
- Dating apps and online dating

Language Style (for sensitive topics, adapt to determined language):
- Use appropriate terms for intimate topics.
- Employ medical/clinical terms when needed.
- Use culturally understood euphemisms where appropriate.
- Match user's comfort level with terminology.
- Be direct but tasteful.
- Avoid crude or explicit language.
- Use mature, adult-appropriate phrasing.

Quality Standards:
- Ensure responses are comprehensive yet concise.
- Double-check spelling and grammar for the language used.
- Provide actionable advice whenever possible.
- Follow up complex explanations with "Umeed he apko samjh agyi hogi?" (Roman Urdu) or similar in the response language.
- Encourage questions with "Agar koi sawal ho to zaror poochiye ðŸ˜Š" (Roman Urdu) or similar in the response language.

Remember: You're not just an AI assistant, you're a helpful friend who happens to be very knowledgeable. Make every interaction meaningful and helpful!
`;
      }
      
      const EvaAvatarHeaderIcon = ({ avatarState }) => {
        const wrapperRef = useRef(null);
        const svgRef = useRef(null);

        useEffect(() => {
          if (!wrapperRef.current || !svgRef.current) return;
          const wrapperElement = wrapperRef.current;
          const svgElement = svgRef.current;
          wrapperElement.classList.remove('avatar-speaking', 'avatar-thinking', 'avatar-happy-state');
          svgElement.style.animation = ''; 
          switch (avatarState) {
            case 'speaking':
              wrapperElement.classList.add('avatar-speaking');
              break;
            case 'thinking':
              wrapperElement.classList.add('avatar-thinking');
              break;
            case 'happy':
              wrapperElement.classList.add('avatar-happy-state');
              svgElement.style.animation = 'happyBounce 0.5s ease-out';
              break;
            case 'idle':
            default:
              break;
          }
        }, [avatarState]);

        return (
          <div ref={wrapperRef} className="avatar-wrapper w-8 h-8 relative"> 
            <svg
              ref={svgRef}
              className="avatar-svg-element w-full h-full relative z-10"
              xmlns="http://www.w3.org/2000/svg"
              shapeRendering="geometricPrecision"
              textRendering="geometricPrecision"
              imageRendering="optimizeQuality"
              fillRule="evenodd"
              clipRule="evenodd"
              viewBox="0 0 512 512"
            >
              <circle fill="#FBD433" transform="matrix(2.6439 -.70843 .70843 2.6439 256 256)" r="93.504" />
              <path
                fillRule="nonzero"
                d="M311.614 167.208c-8.713 1.067-16.645-5.131-17.712-13.845-1.066-8.713 5.131-16.645 13.845-17.712 24.12-3.036 48.201.14 70.414 8.376 22.714 8.421 43.617 22.176 60.713 40.011 6.095 6.337 5.901 16.418-.436 22.514-6.338 6.096-16.419 5.901-22.515-.436-13.713-14.306-30.536-25.359-48.863-32.154-17.694-6.56-36.669-9.118-55.446-6.754zM146.707 359.673c-9.444-14.665-3.044-27.062 12.626-24.418 64.465 17.411 130.028 17.762 196.742 0 10.969-.905 18.587 12.276 9.823 24.418-57.576 65.374-173.661 60.462-219.191 0zM295.93 236.946c-4.687 6.546-8.106 12.348-9.362 16.866-3.147 11.338 7.751 11.19 15.425 6.84 30.38-17.226 72.383-14.695 103.189.004 17.31 8.258 18.633-4.058 8.774-16.619-38.463-49.008-75.435-50.52-118.026-7.091zm-199.097-64.48c-4.201 6.752-13.08 8.821-19.832 4.621-6.751-4.2-8.82-13.08-4.62-19.831 10.745-17.197 30.454-33.148 51.336-45.051 23.625-13.465 49.661-22.103 66.441-21.711 7.932.155 14.239 6.715 14.083 14.648-.155 7.933-6.715 14.239-14.648 14.084-11.904-.279-32.181 6.879-51.679 17.992-17.164 9.783-33.029 22.363-41.081 35.248zm70.322 15.866c21.402 0 38.751 22.179 38.751 49.537s-17.349 49.537-38.751 49.537c-21.402 0-38.751-22.179-38.751-49.537s17.349-49.537 38.751-49.537z"
              />
            </svg>
          </div>
        );
      };

      const ImagePreviewModal = ({ src, onClose, altText }) => { 
        const [isClosing, setIsClosing] = useState(false);

        const handleClose = () => {
            setIsClosing(true);
            setTimeout(() => {
                onClose();
            }, 280); 
        };
        
        useEffect(() => {
            const handleEsc = (event) => {
                if (event.key === 'Escape') {
                    handleClose();
                }
            };
            window.addEventListener('keydown', handleEsc);
            return () => window.removeEventListener('keydown', handleEsc);
        }, [onClose]);


        if (!src) return null;

        return (
            <div
                className={`fixed inset-0 bg-black/80 flex items-center justify-center z-50 p-4
                            ${isClosing ? 'animate-modal-fade-out' : 'animate-modal-fade-in'}`}
                onClick={handleClose} 
            >
                <button
                    onClick={(e) => { e.stopPropagation(); handleClose(); }}
                    className="absolute top-4 right-4 text-white/80 hover:text-white text-4xl font-light z-[51] transition-colors"
                    aria-label="Close image preview"
                >
                    Ã—
                </button>
                <img
                    src={src}
                    alt={altText || "Image Preview"}
                    className="max-w-[95vw] max-h-[95vh] object-contain rounded-lg shadow-2xl"
                    onClick={(e) => e.stopPropagation()} 
                />
            </div>
        );
      };
      
      const COPY_ICON_SVG = `<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-4 h-4"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75" /></svg>`;
      const CHECK_ICON_SVG = `<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" class="w-4 h-4"><path stroke-linecap="round" stroke-linejoin="round" d="M9 12.75L11.25 15 15 9.75M21 12a9 9 0 11-18 0 9 9 0 0118 0z" /></svg>`;
      const FileTextIcon = (props) => (<i className={`fa-solid fa-file-lines ${props.className || ''}`}></i>);
      const FileImageIcon = (props) => (<i className={`fa-solid fa-file-image ${props.className || ''}`}></i>);

      const ChatMessageItem = ({ message, onOpenImagePreview }) => { 
        const isUser = message.role === 'user'; 
        const isError = message.isError === true; 
        const isModelLoadingText = message.role === 'model' && message.isLoading && !isError && !message.isLoadingImage;
        const contentRef = useRef(null); 
        
        const isModelLoadingWithSearch = isModelLoadingText && message.searchUsed; 
        const isModelLoadingWithoutSearch = isModelLoadingText && !message.searchUsed;
        
        useEffect(() => {
          if (message.role === 'model' && !isError && !message.isLoadingImage && contentRef.current) {
            const preElements = contentRef.current.querySelectorAll('pre');
            preElements.forEach(preEl => {
              if (preEl.parentNode && preEl.parentNode.classList.contains('code-block-wrapper')) {
                const existingCodeElement = preEl.querySelector('code');
                if (existingCodeElement && !existingCodeElement.dataset.highlighted) {
                    hljs.highlightElement(existingCodeElement);
                    existingCodeElement.dataset.highlighted = 'true';
                }
                return;
              }

              const codeElement = preEl.querySelector('code');
              if (!codeElement) return;

              const wrapper = document.createElement('div');
              wrapper.classList.add('code-block-wrapper');
              wrapper.style.position = 'relative';

              const button = document.createElement('button');
              button.innerHTML = COPY_ICON_SVG;
              button.classList.add(
                'code-copy-button', 'absolute', 'p-1',
                'bg-transparent', 
                'text-gray-500', 
                'border', 'border-gray-300', 
                'rounded-md', 'transition-colors', 'duration-150',
                'opacity-80', 
                'hover:opacity-100', 'hover:text-gray-700' 
              );
              button.style.top = '0.25rem';
              button.style.right = '0.5rem';
              button.style.zIndex = '10';
              button.setAttribute('aria-label', 'Copy code');

              button.onclick = () => {
                if (codeElement) {
                  navigator.clipboard.writeText(codeElement.textContent || '').then(() => {
                    button.innerHTML = CHECK_ICON_SVG;
                    button.classList.add('text-emerald-600'); 
                    button.classList.remove('text-gray-500', 'text-gray-700'); 
                    setTimeout(() => {
                      button.innerHTML = COPY_ICON_SVG;
                      button.classList.remove('text-emerald-600'); 
                      button.classList.add('text-gray-500'); 
                    }, 2000);
                  }).catch(err => console.error('Failed to copy: ', err));
                }
              };

              if (preEl.parentNode) {
                preEl.parentNode.insertBefore(wrapper, preEl);
                wrapper.appendChild(preEl); 
                wrapper.appendChild(button); 
              }
              
              if (!codeElement.dataset.highlighted) {
                hljs.highlightElement(codeElement);
                codeElement.dataset.highlighted = 'true';
              }
            });
          }
        }, [message.text, message.role, message.isLoading, message.isLoadingImage, isError]);

        if (message.role === 'system') { const systemMessageStyle = isError ? "px-4 py-2 text-xs text-red-700 bg-red-100 border border-red-300 rounded-full shadow-sm italic" : "px-4 py-2 text-xs text-text-subtle bg-surface-accent border border-border-light rounded-full shadow-sm italic"; return (<div className="flex justify-center my-4 animate-fade-in-up opacity-0" style={{animationFillMode: 'forwards'}}><div className={systemMessageStyle}>{isError ? <><i className="fas fa-exclamation-triangle mr-1.5"></i> {message.text}</> : message.text }</div></div>);}
        
        let bubbleBaseClasses = 'px-4 py-3 md:px-4 md:py-3 shadow-card animate-fade-in-up opacity-0'; 
        let bubbleRoleClasses = ''; 
        let bubbleWidthClasses = ''; 
        let contentToShow = null;
        
        if (isUser) { 
          bubbleRoleClasses = 'bg-surface-accent text-text-primary rounded-2xl rounded-br-md'; 
          bubbleWidthClasses = 'max-w-[80%] sm:max-w-[75%]'; 
          contentToShow = <p className="text-base whitespace-pre-wrap break-words">{message.text}</p>; 
        } else if (message.role === 'model') { 
            bubbleRoleClasses = 'bg-white text-text-primary rounded-2xl border border-[#E5E8EB]'; 
            bubbleWidthClasses = 'max-w-full w-full'; 
            
            if (message.isLoadingImage) {
                 return ( 
                    <div className={`flex items-end justify-start animate-fade-in-up opacity-0`} style={{animationFillMode: 'forwards'}}>
                        <div className={`${bubbleBaseClasses} ${bubbleWidthClasses} ${bubbleRoleClasses}`}>
                            <div className="flex items-center space-x-1.5 p-1"> 
                               <svg className="animate-spin h-4 w-4 text-brand-primary" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                                 <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
                                 <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                               </svg>
                              <span className="text-sm font-medium text-text-subtle">Generating...</span>
                            </div>
                        </div>
                    </div>
                );
            } else if (message.generatedImageB64) {
                contentToShow = (
                    <div className="rounded-md overflow-hidden"> {/* Added overflow-hidden */}
                        <img
                            src={`data:image/png;base64,${message.generatedImageB64}`}
                            alt={message.userImageRequestPrompt || "Generated by Oracle AI"}
                            className="block w-full max-h-[26rem] object-cover rounded-md border border-slate-100" 
                        />
                    </div>
                );
            } else if (message.imageGenerationError) {
                bubbleRoleClasses = 'bg-red-50 border border-red-300 text-red-700 rounded-2xl';
                contentToShow = (
                     <div className="text-sm break-words"> 
                        <div className="flex items-center mb-1"><i className="fas fa-exclamation-triangle text-red-500 mr-2"></i><strong className="font-semibold">Image Generation Failed</strong></div>
                        {message.imageGenerationError}
                        {message.text && ( 
                            <p className="text-xs mt-2 text-red-600">{message.text}</p>
                        )}
                    </div>
                );
            } else if(isError) { 
                bubbleRoleClasses = 'bg-red-50 border border-red-300 text-red-700 rounded-2xl'; 
                contentToShow = ( <div className="text-sm break-words"> <div className="flex items-center mb-1"><i className="fas fa-exclamation-triangle text-red-500 mr-2"></i><strong className="font-semibold">Error</strong></div>{message.text.replace(/^Error:\s*/, '')} </div>); 
            } else { 
                contentToShow = (<div ref={contentRef} className="text-base markdown-content break-words" dangerouslySetInnerHTML={{ __html: marked.parse(message.text || '') }} />); 
            } 
        }

        return (
          <div className={`flex items-end ${isUser ? 'justify-end' : 'justify-start'} animate-fade-in-up opacity-0`} style={{animationFillMode: 'forwards'}}>
            <div className={`${bubbleBaseClasses} ${bubbleWidthClasses} ${bubbleRoleClasses}`} style={{animationFillMode: 'forwards'}}>
              {isUser && message.attachedFiles && message.attachedFiles.length > 0 && (
                <div className={`grid gap-2.5 ${message.text ? 'mb-2.5' : ''} ${message.attachedFiles.length > 1 ? 'grid-cols-2' : 'grid-cols-1'}`}>
                  {message.attachedFiles.map(file => {
                    if (file.type?.startsWith('image/') && file.apiData?.data && file.apiData?.mimeType) {
                      const imageSrc = `data:${file.apiData.mimeType};base64,${file.apiData.data}`;
                      return (
                        <div key={file.id || file.name} className="rounded-lg overflow-hidden border border-brand-primary/20 bg-brand-primary/10 shadow-sm group relative max-w-full w-full sm:w-auto sm:max-w-[180px]">
                          <img src={imageSrc} alt={file.name || 'image preview'} className="block w-full max-h-40 object-contain" />
                          <div className="absolute inset-0 bg-black/60 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex items-center justify-center p-1">
                            <p className="text-[10px] text-white text-center truncate font-medium">{file.name}</p>
                          </div>
                        </div>
                      );
                    } else {
                      return (
                        <div key={file.id || file.name} className="bg-brand-primary/10 border border-brand-primary/20 p-2.5 rounded-lg text-xs flex items-center space-x-2 max-w-full w-fit shadow">
                          {file.type?.startsWith('image/') ? 
                            <FileImageIcon className="text-brand-primary text-base" /> : 
                            <FileTextIcon className="text-brand-primary text-base" />
                          }
                          <span className="truncate max-w-[120px] sm:max-w-[150px] text-text-primary font-medium">
                            {file.name}{file.type?.startsWith('image/') ? " (preview unavailable)" : ""}
                          </span>
                        </div>
                      );
                    }
                  })}
                </div>
              )}
              {message.text && !message.generatedImageB64 && !message.imageGenerationError && contentToShow} 
              {message.generatedImageB64 && contentToShow}
              {message.imageGenerationError && contentToShow}

              {isModelLoadingWithoutSearch && ( 
                <div className="flex items-end space-x-1.5 mt-2.5">
                  <span className="w-1.5 h-1.5 bg-text-subtle rounded-full animate-typing-indicator" style={{ animationDelay: '0s' }}></span>
                  <span className="w-1.5 h-1.5 bg-text-subtle rounded-full animate-typing-indicator" style={{ animationDelay: '0.15s' }}></span>
                  <span className="w-1.5 h-1.5 bg-text-subtle rounded-full animate-typing-indicator" style={{ animationDelay: '0.3s' }}></span>
                </div>
              )}
              {isModelLoadingWithSearch && (
                <div className="flex items-center space-x-1.5 mt-2.5 text-text-subtle">
                   <svg className="animate-spin h-3.5 w-3.5 text-brand-primary" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                     <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
                     <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                   </svg>
                  <span className="text-xs font-medium text-text-subtle">Searching...</span>
                </div>
              )}
              {message.role === 'model' && !isError && !isModelLoadingText && !message.isLoadingImage && !message.generatedImageB64 && !message.imageGenerationError && message.citationMetadata && message.citationMetadata.citationSources && message.citationMetadata.citationSources.length > 0 && (
                <div className="mt-3 pt-3 border-t border-[#E5E8EB]/70">
                  <h4 className="text-xs font-semibold text-text-subtle mb-1.5 flex items-center">
                    <i className="fas fa-link text-xs mr-1.5 opacity-70 text-text-primary"></i>Sources:
                  </h4>
                  <ul className="list-none p-0 space-y-1">
                    {message.citationMetadata.citationSources.map((source, index) => (
                      <li key={index} className="text-xs leading-relaxed">
                        <a href={source.uri} target="_blank" rel="noopener noreferrer" className="text-brand-primary hover:text-brand-secondary hover:underline break-all inline-flex items-center group" title={source.uri}>
                          <span className="mr-1">{index + 1}.</span>
                          <span className="truncate max-w-[200px] sm:max-w-[300px] md:max-w-[400px]">{source.title || (source.uri && source.uri.length > 60 ? source.uri.substring(0, 60) + "..." : source.uri)}</span>
                          <i className="fas fa-external-link-alt text-[10px] ml-1.5 opacity-50 group-hover:opacity-100 transition-opacity"></i>
                        </a>
                      </li>
                    ))}
                  </ul>
                </div>
              )}
            </div>
          </div>);
      };
      
      const ChatInterface = ({ messages, onSendMessage, isLoading, onStopGeneration, onRegenerateLastTextMessage, onClearChat, setChatError, avatarHeaderState, isSearchEnabled, onToggleSearch, isImageGenEnabled, onToggleImageGen, onRegenerateImage, onOpenImagePreview }) => {
        const [inputText, setInputText] = useState(''); const [selectedFiles, setSelectedFiles] = useState([]); const [isMobileView, setIsMobileView] = useState(false); const [userHasScrolledUp, setUserHasScrolledUp] = useState(false);
        const fileInputRef = useRef(null); const messagesEndRef = useRef(null); const textareaRef = useRef(null); const chatContainerRef = useRef(null); 
        useEffect(() => { const checkMobileView = () => setIsMobileView(window.innerWidth < 768); window.addEventListener('resize', checkMobileView); checkMobileView(); return () => window.removeEventListener('resize', checkMobileView); }, []);
        useEffect(() => { const container = chatContainerRef.current; if (!container) return; const handleScroll = () => { const { scrollTop, scrollHeight, clientHeight } = container; setUserHasScrolledUp(scrollHeight - scrollTop - clientHeight >= 20);}; container.addEventListener('scroll', handleScroll, { passive: true }); return () => container.removeEventListener('scroll', handleScroll); }, []); 
        
        useEffect(() => {
          const lastMessage = messages[messages.length - 1];
          if (!lastMessage) return; 

          const messagesEndEl = messagesEndRef.current;
          if (!messagesEndEl) return;

          if (lastMessage.role === 'user') {
            messagesEndEl.scrollIntoView({ behavior: "smooth" });
          } else if (lastMessage.role === 'model') {
            if (!userHasScrolledUp) { 
              if (lastMessage.isLoading || lastMessage.isLoadingImage) {
                messagesEndEl.scrollIntoView({ behavior: "auto" }); 
              } else {
                messagesEndEl.scrollIntoView({ behavior: "smooth" });
              }
            }
          }
        }, [messages, userHasScrolledUp]);

        useEffect(() => { const ta = textareaRef.current; if (ta) { ta.style.height = 'auto'; const scrollHeight = ta.scrollHeight; const maxHeight = parseInt(getComputedStyle(ta).maxHeight, 10) || 180; ta.style.height = `${Math.min(scrollHeight, maxHeight)}px`; }}, [inputText]);
        const handleFileChange = async (event) => { const files = Array.from(event.target.files); processAndSetFiles(files); if (fileInputRef.current) fileInputRef.current.value = ""; };
        const handlePaste = async (event) => { const pastedFiles = Array.from(event.clipboardData.files); if (pastedFiles.length > 0) { const processedSomething = await processAndSetFiles(pastedFiles, true); if (processedSomething) event.preventDefault(); }};
        const processAndSetFiles = async (filesArray, isFromPaste = false) => { if (!filesArray.length) return false; let filesToProcess = []; let currentTotalSize = selectedFiles.reduce((acc, curr) => acc + (curr.file?.size || 0), 0); let didProcessAnyFile = false; let localErrorMessage = null; for (const file of filesArray) { const isImage = file.type.startsWith('image/'); const isTxt = file.type === 'text/plain' || (file.name && file.name.toLowerCase().endsWith('.txt')); if (!isImage && !isTxt) { if (isFromPaste) console.log(`Pasted unsupported file: ${file.name} (${file.type})`); else localErrorMessage = `File "${file.name}" (type: ${file.type}) is not supported. Only images and .txt files allowed.`; continue; } if (file.size > MAX_FILE_SIZE_MB * 1024 * 1024) { localErrorMessage = `File "${file.name}" is larger than ${MAX_FILE_SIZE_MB}MB.`; continue; } if (currentTotalSize + file.size > MAX_TOTAL_FILES_SIZE_MB * 1024 * 1024) { localErrorMessage = `Total file size cannot exceed ${MAX_TOTAL_FILES_SIZE_MB}MB.`; break; } filesToProcess.push(file); currentTotalSize += file.size; didProcessAnyFile = true; } if(localErrorMessage){ setChatError(localErrorMessage); } else { setChatError(null); } if (filesToProcess.length > 0) { const processedFileObjects = await Promise.all( filesToProcess.map(async (file) => { const id = Date.now().toString(36) + Math.random().toString(36).substring(2) + file.name; let apiData; if (file.type.startsWith('image/')) { const base64Data = await new Promise((resolve, reject) => { const reader = new FileReader(); reader.readAsDataURL(file); reader.onloadend = () => resolve(reader.result.split(',')[1]); reader.onerror = (error) => reject(error); }); apiData = { mimeType: file.type, data: base64Data }; } else { const textContent = await file.text(); apiData = { textContent: textContent }; } return { id, file, name: file.name, type: file.type, apiData }; }) ); setSelectedFiles(prev => [...prev, ...processedFileObjects.filter(f => f)]); } return didProcessAnyFile; };
        const removeSelectedFile = (fileId) => { setSelectedFiles(prev => prev.filter(f => f.id !== fileId)); };
        const handleSubmit = (e) => { if(e) e.preventDefault(); if (!inputText.trim() && selectedFiles.length === 0) return; setUserHasScrolledUp(false); setChatError(null); onSendMessage(inputText, selectedFiles); setInputText(''); setSelectedFiles([]); };
        const handleTextareaKeyDown = (e) => { if (e.key === 'Enter' && !e.shiftKey) { if (isMobileView) { return; } else { if (!isSendButtonDisabled) { handleSubmit(e); } else { e.preventDefault(); }}}};
        const isAiCurrentlyProcessing = messages.some(m => m.role === 'model' && (m.isLoading || m.isLoadingImage) && !m.isError);
        const isSendButtonDisabled = (!isAiCurrentlyProcessing && (isLoading || (inputText.trim() === '' && selectedFiles.length === 0)));
        
        const hasVisibleChatContent = messages.some( msg => (msg.text || msg.attachedFiles?.length > 0 || msg.generatedImageB64 || msg.imageGenerationError) && !(msg.role === 'system' && !msg.isError) );

        const lastMessage = messages[messages.length - 1];
        const userMessageForLastAi = messages.length > 1 ? messages[messages.length - 2] : null;

        const isLastMessageAiAndFinalized = lastMessage && lastMessage.role === 'model' &&
                                          !lastMessage.isLoading && !lastMessage.isLoadingImage &&
                                          !isAiCurrentlyProcessing;

        const canRegenerateLastAiMessageOverall = isLastMessageAiAndFinalized && userMessageForLastAi && userMessageForLastAi.role === 'user';

        const canRegenerateLastAiAsText = canRegenerateLastAiMessageOverall &&
                                          !(lastMessage.generatedImageB64 || lastMessage.imageGenerationError) && 
                                          !userMessageForLastAi.attachedFiles?.some(f => f.type?.startsWith('image/')); 

        const canRegenerateLastAiAsImage = canRegenerateLastAiMessageOverall &&
                                           (lastMessage.generatedImageB64 || lastMessage.imageGenerationError);


        return (
          <div className="w-full h-full sm:max-w-2xl md:max-w-3xl lg:max-w-[780px] sm:mx-auto bg-white rounded-none sm:rounded-xl shadow-chat-window flex flex-col overflow-hidden sm:my-6 sm:max-h-[calc(100vh-3rem)]">
            <header className="p-4 border-b border-border-default bg-white flex items-center justify-between flex-shrink-0 shadow-header">
                <div className="flex items-center space-x-3">
                    <EvaAvatarHeaderIcon avatarState={avatarHeaderState} />
                    <div>
                        <h1 className="text-xl font-roboto font-semibold tracking-tight text-gradient-brand">Oracle AI</h1>
                        <p className="text-xs -mt-0.5 font-roboto text-gradient-brand-subtle">Modern Chat Experience</p>
                    </div>
                </div>
                <button onClick={() => { setChatError(null); onClearChat(); }} className="text-sm font-medium text-text-secondary hover:text-text-primary px-3 py-1.5 rounded-lg hover:bg-interactive-hover transition-colors duration-150 disabled:opacity-50 flex items-center space-x-1.5 group" aria-label="Clear chat" disabled={isLoading || messages.filter(m=>m.text || m.attachedFiles?.length > 0 || m.generatedImageB64).length === 0}><i className="fas fa-broom fa-fw text-sm transition-transform group-hover:scale-110"></i><span className="hidden sm:inline">Clear</span></button>
            </header>
            
            <div ref={chatContainerRef} className="flex-grow p-4 md:p-5 space-y-4 md:space-y-5 overflow-y-auto chat-messages-container relative bg-gray-50">
                { !hasVisibleChatContent && !isLoading && ( 
                    <div className="absolute inset-0 flex flex-col items-center justify-center text-center pointer-events-none p-8 opacity-0 animate-fade-in-up" style={{animationDelay: '0.2s', animationFillMode: 'forwards'}}>
                        <p className="text-xl md:text-2xl font-[Outfit] font-semibold mb-2" style={{ color: '#536872' }}>Hello there!</p>
                        <p className="text-sm font-[Outfit] mt-1.5" style={{ color: '#536872' }}>I am Oracle AI. You can ask me anything or share files to get started.</p>
                    </div> 
                )}
                {messages.map((msg) => ( (msg.text || msg.attachedFiles?.length > 0 || (msg.role === 'model' && (msg.isLoading || msg.isLoadingImage)) || msg.isError || msg.generatedImageB64 || msg.imageGenerationError) && 
                    <div key={msg.id}> 
                        <ChatMessageItem 
                            message={msg} 
                            onOpenImagePreview={onOpenImagePreview}
                        /> 
                    </div> 
                ))}
                {(canRegenerateLastAiAsText || canRegenerateLastAiAsImage) && (
                    <div className={`flex mt-2 justify-start`}> 
                        <button 
                            onClick={() => { 
                                setChatError(null); 
                                if (canRegenerateLastAiAsImage) {
                                    onRegenerateImage(lastMessage.id);
                                } else if (canRegenerateLastAiAsText) {
                                    onRegenerateLastTextMessage(lastMessage.id);
                                }
                            }} 
                            className="p-1.5 rounded-full text-text-subtle hover:text-brand-primary hover:bg-interactive-hover transition-all duration-150 group" 
                            aria-label="Regenerate response" 
                            disabled={isLoading || isAiCurrentlyProcessing} 
                        > 
                            <i className="fas fa-arrows-rotate text-xs transform transition-transform duration-200 group-hover:rotate-[75deg]"></i> 
                        </button> 
                    </div> 
                )}
                <div ref={messagesEndRef} className="h-1" />
            </div>
            
            {selectedFiles.length > 0 && ( <div className="p-3 border-t border-border-default bg-white flex flex-wrap gap-2 items-center overflow-x-auto flex-shrink-0"> {selectedFiles.map(file => ( <div key={file.id} className="bg-gray-100 border border-border-light text-xs pl-2.5 pr-1.5 py-1 rounded-full flex items-center shadow-sm hover:shadow-md transition-shadow duration-150 flex-shrink-0 animate-scale-in-subtle"> {file.type.startsWith('image/') ? <FileImageIcon className="text-brand-primary text-sm mr-1.5"/> : <FileTextIcon className="text-brand-primary text-sm mr-1.5"/> } <span className="truncate max-w-[90px] sm:max-w-[120px] text-text-secondary font-medium">{file.name}</span> <button onClick={() => removeSelectedFile(file.id)} className="ml-2 text-slate-400 hover:text-status-error p-0.5 rounded-full hover:bg-red-500/10 transition-colors"> <i className="fas fa-times text-xs"></i> </button> </div> ))} </div> )}

            <form onSubmit={handleSubmit} className="bg-white border-t border-border-default flex-shrink-0">
              <div className="px-3 md:px-4 pt-2 pb-1 flex justify-start items-center space-x-2">
                <button
                  type="button"
                  onClick={onToggleSearch}
                  disabled={isLoading || isAiCurrentlyProcessing || isImageGenEnabled}
                  className={`flex items-center space-x-1.5 px-2.5 py-1 rounded-md text-xs font-medium transition-all duration-200 ease-in-out group
                              focus:outline-none 
                              disabled:opacity-60 disabled:cursor-not-allowed
                              ${isSearchEnabled 
                                  ? 'bg-blue-100 hover:bg-blue-200' 
                                  : 'bg-gray-100 hover:bg-gray-200'
                              }`}
                  title={isSearchEnabled ? "Web Search ON" : "Web Search OFF"}
                >
                  <i className={`fas fa-globe fa-fw transition-colors duration-200 
                                ${isSearchEnabled 
                                    ? 'text-blue-600' 
                                    : 'text-gray-500 group-hover:text-gray-600'
                                }`}></i>
                  <span className={`transition-colors duration-200 
                                  ${isSearchEnabled 
                                      ? 'text-blue-700' 
                                      : 'text-text-subtle group-hover:text-text-secondary'
                                  }`}>Search</span>
                </button>
                <button
                  type="button"
                  onClick={onToggleImageGen}
                  disabled={isLoading || isAiCurrentlyProcessing || isSearchEnabled}
                  className={`flex items-center space-x-1.5 px-2.5 py-1 rounded-md text-xs font-medium transition-all duration-200 ease-in-out group
                              focus:outline-none
                              disabled:opacity-60 disabled:cursor-not-allowed
                              ${isImageGenEnabled
                                  ? 'bg-purple-100 hover:bg-purple-200'
                                  : 'bg-gray-100 hover:bg-gray-200'
                              }`}
                  title={isImageGenEnabled ? "Image Generation ON" : "Image Generation OFF"}
                >
                  <i className={`fas fa-image fa-fw transition-colors duration-200
                                ${isImageGenEnabled
                                    ? 'text-purple-600'
                                    : 'text-gray-500 group-hover:text-gray-600'
                                }`}></i>
                  <span className={`transition-colors duration-200
                                  ${isImageGenEnabled
                                      ? 'text-purple-700'
                                      : 'text-text-subtle group-hover:text-text-secondary'
                                  }`}>Gen</span>
                </button>
              </div>

              <div className="pt-2 pb-3 pr-3 md:pt-2 md:pb-4 md:pr-4 flex items-end space-x-2 md:space-x-3">
                <input type="file" ref={fileInputRef} onChange={handleFileChange} className="hidden" multiple accept="image/*,.txt,text/plain" disabled={isLoading || isAiCurrentlyProcessing} /> 
                <button
                  type="button"
                  onClick={() => fileInputRef.current?.click()}
                  className="
                    w-10 h-10 bg-white border border-border-medium rounded-xl
                    flex items-center justify-center text-text-subtle
                    shadow-sm
                    hover:bg-surface-accent 
                    transition-colors duration-150
                    disabled:opacity-60 disabled:cursor-not-allowed
                    flex-shrink-0
                  "
                  aria-label="Attach files"
                  disabled={isLoading || isAiCurrentlyProcessing}
                >
                  <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M12 6v6m0 0v6m0-6h6m-6 0H6"/>
                  </svg>
                </button>
                <textarea 
                    ref={textareaRef} 
                    value={inputText} 
                    onChange={(e) => setInputText(e.target.value)} 
                    onPaste={handlePaste} 
                    onKeyDown={handleTextareaKeyDown} 
                    placeholder="Type your message..." 
                    rows={1} 
                    className="flex-grow px-2.5 sm:px-3 py-3 text-sm md:text-base border border-border-medium rounded-xl bg-white shadow-sm focus:outline-none focus:ring-1 focus:ring-interactive-focus-ring focus:border-transparent resize-none placeholder-text-subtle auto-resize" 
                    disabled={isLoading || isAiCurrentlyProcessing}
                /> 
                <button 
                    type={isAiCurrentlyProcessing ? "button" : "submit"} 
                    onClick={isAiCurrentlyProcessing ? onStopGeneration : undefined } 
                    disabled={isAiCurrentlyProcessing ? false : isSendButtonDisabled} 
                    className={`
                        font-semibold w-10 h-10 rounded-xl shadow-sm 
                        flex items-center justify-center 
                        transition-all duration-200 ease-in-out transform active:animate-button-press 
                        flex-shrink-0 focus:outline-none
                        ${ (isSendButtonDisabled && !isAiCurrentlyProcessing) 
                            ? 'bg-gray-200 text-gray-400 cursor-not-allowed shadow-none' 
                            : `bg-button-send-bg text-text-on-primary hover:bg-button-send-hover-bg ${!isAiCurrentlyProcessing ? 'focus:ring-2 focus:ring-offset-2 focus:ring-interactive-focus-ring' : ''}`
                        }
                    `}
                    aria-label={isAiCurrentlyProcessing ? "Stop generation" : "Send message"}
                > 
                  {isAiCurrentlyProcessing ? (
                    <i className="fas fa-stop text-sm"></i> 
                  ) : (
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" className="w-5 h-5">
                      <path d="M3.478 2.404a.75.75 0 0 0-.926.941l2.432 7.905H13.5a.75.75 0 0 1 0 1.5H4.984l-2.432 7.905a.75.75 0 0 0 .926.94 60.519 60.519 0 0 0 18.445-8.986.75.75 0 0 0 0-1.218A60.517 60.517 0 0 0 3.478 2.404Z" />
                    </svg>
                  )} 
                </button> 
              </div>
            </form>
          </div>);
      };
      
      const App = () => { 
        const [chatMessages, setChatMessages] = useState([]); 
        const [isChatLoading, setIsChatLoading] = useState(false); 
        const [chatError, setChatError] = useState(null); 
        const [avatarHeaderState, setAvatarHeaderState] = useState('idle'); 
        const [isSearchEnabled, setIsSearchEnabled] = useState(false); 
        const [isImageGenEnabled, setIsImageGenEnabled] = useState(false);
        const [previewImage, setPreviewImage] = useState({ src: null, alt: null }); 

        const isStoppingGenerationRef = useRef(false);
        const searchWasActiveForLastUserTurnRef = useRef(false); 
        const imageGenWasActiveForLastUserTurnRef = useRef(false);


        useEffect(() => { 
            if (chatError) { 
                const errorId = Date.now() + '_sys_validation_err'; 
                setChatMessages(prev => { 
                    const lastMessage = prev[prev.length -1]; 
                    if(lastMessage && lastMessage.id.includes('_sys_validation_err') && lastMessage.text === chatError) return prev; 
                    return [...prev, {id: errorId, role: 'system', text: chatError, isError: true, timestamp: new Date()}]; 
                }); 
                setAvatarHeaderState('idle');
            }
        }, [chatError]); 

        const getGeminiHistory = (currentMessagesInternalFormat) => {
            const geminiMessages = currentMessagesInternalFormat
                .filter(msg => !(msg.role === 'system' || (msg.role === 'model' && (msg.isLoading || msg.isLoadingImage)) || msg.isError))
                .map(msg => {
                    const parts = [];
                    if (msg.role === 'model') {
                        if (msg.generatedImageB64) {
                            parts.push({ text: `[Internal Note: You generated an image for the user's request: '${msg.userImageRequestPrompt || 'previous request'}'.]` });
                        } else if (msg.imageGenerationError) {
                             parts.push({ text: `[Internal Note: Image generation failed for user's request: '${msg.userImageRequestPrompt || 'previous request'}'. Error: ${msg.imageGenerationError}]` });
                        } else if (msg.text) { // Regular text message from model
                            parts.push({ text: msg.text });
                        }
                    } else if (msg.role === 'user') {
                        if (msg.text) parts.push({ text: msg.text });
                        if (msg.attachedFiles) {
                            msg.attachedFiles.forEach(file => {
                                if (file.type.startsWith('image/') && file.apiData?.mimeType && file.apiData?.data) {
                                    parts.push({ inlineData: { mimeType: file.apiData.mimeType, data: file.apiData.data } });
                                } else if ((file.name.toLowerCase().endsWith('.txt') || file.type === 'text/plain') && file.apiData?.textContent) {
                                    parts.push({ text: `\n[Attached Text File: ${file.name}]\n${file.apiData.textContent}\n` });
                                }
                            });
                        }
                    }
                    return { role: msg.role, parts };
                }).filter(entry => entry.parts.length > 0);

            if (geminiMessages.length > 0 && geminiMessages[0].role === 'model') {
                const firstUserIndex = geminiMessages.findIndex(m => m.role === 'user');
                if (firstUserIndex === -1) return []; 
                const slicedHistory = geminiMessages.slice(firstUserIndex);
                if (slicedHistory.length > 1 && slicedHistory[0].role === slicedHistory[1].role) {
                    return [slicedHistory[0]];
                }
                return slicedHistory;
            }
            return geminiMessages;
        };
        useEffect(() => { if (!API_KEY || API_KEY === "YOUR_GEMINI_API_KEY") { setChatError("API Key is not configured. Please set it up."); } setAvatarHeaderState('idle'); }, []); 
        
        const handleStopGeneration = useCallback(() => {
            isStoppingGenerationRef.current = true;
            setIsChatLoading(false); 
            setAvatarHeaderState('idle');

            setChatMessages(prevMessages => {
                let messageUpdated = false;
                const updatedMessages = prevMessages.map(msg => {
                    if (msg.role === 'model' && (msg.isLoading || msg.isLoadingImage)) {
                        messageUpdated = true;
                        let newText = msg.text || ""; 
                        const stopNotice = "*Generation stopped by user.*";

                        if (msg.isLoadingImage) { 
                             return { ...msg, isLoadingImage: false, imageGenerationError: stopNotice, text: "", userImageRequestPrompt: msg.userImageRequestPrompt || "Unknown prompt" };
                        }
                        
                        if (!newText.includes(stopNotice)) {
                            const currentContent = msg.text || "";
                            if (currentContent.trim()) {
                                let processedContent = currentContent;
                                const isInCodeBlock = (processedContent.match(/```/g) || []).length % 2 !== 0;
                                if (isInCodeBlock) {
                                    processedContent += "\n```"; 
                                }
                                newText = processedContent + (processedContent.trim() ? "\n\n---\n" : "") + stopNotice;
                            } else {
                                newText = stopNotice;
                            }
                        }
                        
                        return {
                            ...msg,
                            text: newText,
                            isLoading: false, 
                            isLoadingImage: false,
                        };
                    }
                    return msg;
                });
                return messageUpdated ? updatedMessages : prevMessages;
            });
            if (searchWasActiveForLastUserTurnRef.current) {
                setIsSearchEnabled(false);
                searchWasActiveForLastUserTurnRef.current = false; 
            }
            if (imageGenWasActiveForLastUserTurnRef.current) {
                setIsImageGenEnabled(false);
                imageGenWasActiveForLastUserTurnRef.current = false;
            }
        }, []); 

        const handleClearChat = useCallback(() => { if(!API_KEY || API_KEY === "YOUR_GEMINI_API_KEY"){ setChatError("API Key not configured. Please set it up to use the chat."); } else { setChatError(null); } setChatMessages([]); setAvatarHeaderState('idle'); setIsSearchEnabled(false); setIsImageGenEnabled(false); searchWasActiveForLastUserTurnRef.current = false; imageGenWasActiveForLastUserTurnRef.current = false; }, []);
        
        const handleToggleSearch = useCallback(() => {
            setIsSearchEnabled(prev => {
                const newState = !prev;
                if (newState) setIsImageGenEnabled(false);
                return newState;
            });
        }, []);

        const handleToggleImageGen = useCallback(() => {
            setIsImageGenEnabled(prev => {
                const newState = !prev;
                if (newState) setIsSearchEnabled(false);
                return newState;
            });
        }, []);

        const handleOpenImagePreview = (src, alt) => { setPreviewImage({ src, alt }); };
        const handleCloseImagePreview = () => { setPreviewImage({ src: null, alt: null }); };

        const mergeCitations = (existingCitations, newCitationSources) => { if (!newCitationSources || newCitationSources.length === 0) return existingCitations; const existingUris = new Set(existingCitations.map(c => c.uri)); const uniqueNewSources = newCitationSources.filter(src => !existingUris.has(src.uri)); return [...existingCitations, ...uniqueNewSources]; };
        
        const triggerImageGenerationProcess = async (imagePromptText, messageIdToUpdate) => {
            console.log("Oracle AI: Starting image generation for prompt:", imagePromptText);
            setIsChatLoading(true); 
            setAvatarHeaderState('thinking');
            setChatMessages(prev => prev.map(m =>
                m.id === messageIdToUpdate ?
                { ...m, isLoading: false, isLoadingImage: true, imageGenerationError: null, generatedImageB64: null, text: '', userImageRequestPrompt: imagePromptText } : m
            ));

            try {
                if (!imagePromptText || imagePromptText.length < 3) {
                    throw new Error("Image prompt is too short or empty.");
                }

                const translationPrompt = `Translate the following text to English if it's not already in English. If it's already in English, return it unchanged. Output only the translated text: "${imagePromptText}"`;
                const translatedTextRaw = await makeApiCallForOracleAI(IMAGE_GEN_TEXT_MODEL_NAME, translationPrompt, null, false);
                const translatedText = translatedTextRaw.replace(/^"(.*)"$/, '$1').trim();

                if (!translatedText) throw new Error("Translation resulted in an empty prompt.");
                if (isStoppingGenerationRef.current) throw new Error("Image generation stopped by user during translation.");

                const enhancedPrompt = await makeApiCallForOracleAI(PROMPT_ENHANCEMENT_MODEL_NAME, translatedText, REFINE_SYSTEM_INSTRUCTION_REALISTIC, false);
                if (!enhancedPrompt || enhancedPrompt.toLowerCase().includes("please provide") || enhancedPrompt.toLowerCase().includes("i need a prompt")) {
                     throw new Error(`Failed to enhance prompt. Model said: "${enhancedPrompt}"`);
                }
                if (isStoppingGenerationRef.current) throw new Error("Image generation stopped by user during prompt enhancement.");
                
                const generatedImageDataB64 = await makeApiCallForOracleAI(IMAGE_GEN_IMAGE_MODEL_NAME, enhancedPrompt, null, true); 
                if (isStoppingGenerationRef.current) throw new Error("Image generation stopped by user during image creation.");

                setChatMessages(prev => prev.map(m =>
                    m.id === messageIdToUpdate ?
                    { ...m, isLoadingImage: false, generatedImageB64: generatedImageDataB64, imageGenerationError: null, text: "", userImageRequestPrompt: imagePromptText } : m
                ));
                setAvatarHeaderState('speaking'); 
                setTimeout(() => setAvatarHeaderState('idle'), 1500);

            } catch (err) {
                console.error("Oracle AI: Image Generation Pipeline Error:", err);
                const errorMsg = err.message.includes("stopped by user") ? err.message : (err.message || "An unknown error occurred during image generation.");
                setChatMessages(prev => prev.map(m =>
                    m.id === messageIdToUpdate ?
                    { ...m, isLoadingImage: false, imageGenerationError: errorMsg, generatedImageB64: null, text: '', userImageRequestPrompt: imagePromptText } : m
                ));
                setAvatarHeaderState('idle');
            } finally {
                setIsChatLoading(false);
                 if (imageGenWasActiveForLastUserTurnRef.current) {
                    setIsImageGenEnabled(false); 
                    imageGenWasActiveForLastUserTurnRef.current = false;
                }
            }
        };

        const _streamAiResponse = async (
            originalPromptParts, 
            targetAiMessageId, 
            historyForGeminiInit, 
            useSearchInitially, 
            isRetryingWithSearch = false, 
            originalUserMessageForRetry = null 
        ) => {
            setIsChatLoading(true);
            if (!isRetryingWithSearch) { 
                setAvatarHeaderState('thinking');
            }

            let displayedAiText = "";
            let accumulatedCitationSources = [];
            let firstChunkProcessed = false;
            let needsSearchMarkerFound = false;
            let needsImageGenMarkerFound = false;


            if (isStoppingGenerationRef.current) return; 

            try {
                const modelConfig = { model: GEMINI_MODEL_NAME, systemInstruction: getSystemInstruction() };
                if (useSearchInitially || isRetryingWithSearch) { 
                    modelConfig.tools = [{ googleSearch: {} }];
                }
                const chatModel = genAI.getGenerativeModel(modelConfig);
                const chatSession = chatModel.startChat({ history: historyForGeminiInit });
                
                if (isStoppingGenerationRef.current) return;

                const result = await chatSession.sendMessageStream(originalPromptParts);

                for await (const chunk of result.stream) { 
                    if (isStoppingGenerationRef.current) break;
                    
                    const rawChunkText = chunk.text();
                    let chunkTextForDisplay = "";

                    if (typeof rawChunkText === 'string') {
                        const lines = rawChunkText.split('\n');
                        const filteredLines = lines.filter(line => 
                            !line.trim().startsWith("tool_code") &&
                            !line.trim().startsWith("print(google_search.search")
                        );
                        if (filteredLines.length > 0 && filteredLines.some(line => line.trim() !== "" || lines.length === filteredLines.length )) {
                           chunkTextForDisplay = filteredLines.join('\n');
                        } else if (filteredLines.length === 0 && rawChunkText.trim() !== "") {
                            chunkTextForDisplay = ""; 
                        } else {
                            chunkTextForDisplay = rawChunkText; 
                        }
                    }

                    if (!firstChunkProcessed && !isRetryingWithSearch) {
                        if (chunkTextForDisplay && chunkTextForDisplay.trim() === "[NEEDS_SEARCH]") {
                            needsSearchMarkerFound = true;
                            setChatMessages(prev => prev.map(m => m.id === targetAiMessageId ? { ...m, isLoading: true, isError: false, searchUsed: true, text: '' } : m));
                            break; 
                        }
                        if (chunkTextForDisplay && chunkTextForDisplay.trim() === "[NEEDS_IMAGE_GEN]") {
                            needsImageGenMarkerFound = true;
                             setChatMessages(prev => prev.map(m => m.id === targetAiMessageId ? { 
                                ...m, 
                                isLoading: false, 
                                isLoadingImage: true, 
                                text: '', 
                                userImageRequestPrompt: originalUserMessageForRetry?.parts.find(p=>p.text)?.text || "Image request"
                             } : m));
                            break;
                        }
                    }
                    firstChunkProcessed = true;


                    if (chunkTextForDisplay.length > 0 || (chunkTextForDisplay === "" && rawChunkText === "")) { 
                        displayedAiText += chunkTextForDisplay;
                    }
                    
                    if (chunk.citationMetadata && chunk.citationMetadata.citationSources) {
                        accumulatedCitationSources = mergeCitations(accumulatedCitationSources, chunk.citationMetadata.citationSources);
                    }

                    if ((displayedAiText.trim().length > 0 || accumulatedCitationSources.length > 0) && !needsSearchMarkerFound && !needsImageGenMarkerFound && !isStoppingGenerationRef.current) {
                        setChatMessages(prev => prev.map(m => m.id === targetAiMessageId ? { ...m, text: displayedAiText, isLoading: true, isError: false, citationMetadata: accumulatedCitationSources.length > 0 ? { citationSources: accumulatedCitationSources } : null, searchUsed: useSearchInitially || isRetryingWithSearch } : m));
                    }
                    
                    if (isStoppingGenerationRef.current) break;
                }

                if (needsSearchMarkerFound && originalUserMessageForRetry && !isRetryingWithSearch) {
                    if (isStoppingGenerationRef.current) return;
                    await _streamAiResponse(originalUserMessageForRetry.parts, targetAiMessageId, historyForGeminiInit, true, true, originalUserMessageForRetry); 
                    return; 
                }

                if (needsImageGenMarkerFound && originalUserMessageForRetry) {
                    if (isStoppingGenerationRef.current) return;
                    const imagePromptText = originalUserMessageForRetry.parts.find(p => p.text)?.text || "Generate an image based on previous context.";
                    await triggerImageGenerationProcess(imagePromptText, targetAiMessageId);
                    return; 
                }
                
                setChatMessages(prev => prev.map(m => {
                    if (m.id === targetAiMessageId) {
                        let finalText = displayedAiText;
                        let isMessageError = false; 
                        const stopNoticeStr = "*Generation stopped by user.*";

                        if (isStoppingGenerationRef.current && !(m.text && m.text.includes(stopNoticeStr)) ) { 
                            let baseText = displayedAiText;
                            const isInCodeBlock = (baseText.match(/```/g) || []).length % 2 !== 0;
                            if (isInCodeBlock) baseText += "\n```";
                            baseText += (baseText.trim() ? "\n\n---\n" : "") + stopNoticeStr;
                            finalText = baseText;
                        } else if (isStoppingGenerationRef.current) {
                             finalText = m.text || stopNoticeStr; 
                        } else if (!finalText.trim() && accumulatedCitationSources.length === 0 && !needsSearchMarkerFound && !needsImageGenMarkerFound) {
                            if (useSearchInitially || isRetryingWithSearch) {
                                finalText = "Mujhe abhi search karne mein thori dushwari pesh aa rahi hai, ya shayad is mauzu par wazeh maloomat nahi mil pa rahi. Aap message dobara regenerate karne ki koshish kar sakte hain, ya apna sawal thora badal kar poochein. Shukriya! ðŸ™";
                            } else {
                                finalText = "Mera khayal hai ke mujhe samajhne mein kuch dushwari hui. Kya aap apna sawal wazeh kar sakte hain? ðŸ¤”";
                            }
                        } else if (!finalText.trim() && accumulatedCitationSources.length > 0 && !isStoppingGenerationRef.current) { 
                            finalText = "*Mawad search results par mabni hai (neeche hawale dekhein).*";
                        }
                        return { ...m, text: finalText, isLoading: false, isError: isMessageError, citationMetadata: accumulatedCitationSources.length > 0 ? { citationSources: accumulatedCitationSources } : null, searchUsed: useSearchInitially || isRetryingWithSearch || m.searchUsed };
                    }
                    return m;
                }));

            } catch (err) {
                console.error("AI stream error in _streamAiResponse:", err);
                let errorTextForUser = "Jawab dene mein kuch takneeki dushwari pesh aa rahi hai. Barah-e-karam thori dair baad dobara koshish karein."; 
                if (err.message?.toLowerCase().includes("api key not valid")) errorTextForUser = "Lagta hai API key mein koi masla hai.";
                else if (err.message?.toLowerCase().includes("candidate was blocked")) errorTextForUser = "Meri hifazati policies ki wajah se main yeh jawab nahi de sakta.";
                else if (err.message?.toLowerCase().includes("quota")) errorTextForUser = "API istemal ki had poori ho gayi hai.";
                
                setChatMessages(prev => prev.map(m => m.id === targetAiMessageId ? { ...m, role: 'model', text: errorTextForUser, isLoading: false, isLoadingImage: false, isError: true, searchUsed: useSearchInitially || isRetryingWithSearch } : m));
            } finally {
                const isFinalResolution = !(needsSearchMarkerFound && !isRetryingWithSearch && !isStoppingGenerationRef.current) && !(needsImageGenMarkerFound && !isStoppingGenerationRef.current);


                if (isFinalResolution) {
                    setIsChatLoading(false);
                    if (!isStoppingGenerationRef.current) { 
                        setAvatarHeaderState('speaking');
                        setTimeout(() => setAvatarHeaderState('idle'), 1500);
                    } else {
                        setAvatarHeaderState('idle'); 
                    }
                    
                    if (searchWasActiveForLastUserTurnRef.current) {
                        setIsSearchEnabled(false);
                        searchWasActiveForLastUserTurnRef.current = false; 
                    }
                     if (imageGenWasActiveForLastUserTurnRef.current) { 
                        setIsImageGenEnabled(false);
                        imageGenWasActiveForLastUserTurnRef.current = false;
                    }
                }
            }
        };

        const handleSendChatMessage = useCallback(async (messageText, filesToSend) => {
            if (!messageText.trim() && filesToSend.length === 0) return;
            isStoppingGenerationRef.current = false; 
            setChatError(null);
            if (!API_KEY || API_KEY === "YOUR_GEMINI_API_KEY") {
                setChatError("API Key is not configured.");
                return;
            }
            
            const currentChatMessages = chatMessages; 
            const userMessageFiles = filesToSend.map(f => ({ id: f.id, name: f.name, type: f.type, apiData: f.apiData }));
            
            const promptPartsForUserTurn = [];
            if (messageText.trim()) promptPartsForUserTurn.push({ text: messageText.trim() });
            filesToSend.forEach(file => {
                if (file.type.startsWith('image/') && file.apiData.mimeType && file.apiData.data) {
                    promptPartsForUserTurn.push({ inlineData: file.apiData });
                } else if ((file.name.toLowerCase().endsWith('.txt') || file.type === 'text/plain') && file.apiData.textContent) {
                    promptPartsForUserTurn.push({ text: `\n[Start of Text File: ${file.name}]\n${file.apiData.textContent}\n[End of Text File: ${file.name}]\n` });
                }
            });
            
            const originalUserMessageForRetry = { role: 'user', parts: promptPartsForUserTurn };
            const newUserMessageForUi = { id: Date.now().toString() + '_user', role: 'user', text: messageText, timestamp: new Date(), attachedFiles: userMessageFiles };
            
            const aiMessageId = Date.now().toString() + '_model_resp';

            if (isImageGenEnabled) {
                imageGenWasActiveForLastUserTurnRef.current = true;
                searchWasActiveForLastUserTurnRef.current = false;

                const aiImagePlaceholder = { 
                    id: aiMessageId, 
                    role: 'model', 
                    timestamp: new Date(), 
                    isLoading: false, 
                    isLoadingImage: true, 
                    userImageRequestPrompt: messageText.trim(),
                    text: '' 
                };
                setChatMessages(prev => {
                    const filteredPrev = prev.filter(m => !(m.role === 'system' && m.isError));
                    return [...filteredPrev, newUserMessageForUi, aiImagePlaceholder];
                });
                await triggerImageGenerationProcess(messageText.trim(), aiMessageId);

            } else { 
                searchWasActiveForLastUserTurnRef.current = isSearchEnabled;
                imageGenWasActiveForLastUserTurnRef.current = false;

                const aiPlaceholder = { 
                    id: aiMessageId, 
                    role: 'model', 
                    text: '', 
                    timestamp: new Date(), 
                    isLoading: true, 
                    isError: false, 
                    citationMetadata: null, 
                    searchUsed: isSearchEnabled,
                    isLoadingImage: false
                }; 
                setChatMessages(prev => {
                    const filteredPrev = prev.filter(m => !(m.role === 'system' && m.isError));
                    return [...filteredPrev, newUserMessageForUi, aiPlaceholder];
                });

                if (promptPartsForUserTurn.length > 0) {
                    const historyForGeminiInit = getGeminiHistory(currentChatMessages);
                    await _streamAiResponse(
                        promptPartsForUserTurn, 
                        aiMessageId, 
                        historyForGeminiInit, 
                        isSearchEnabled, 
                        false, 
                        originalUserMessageForRetry
                    );
                } else { 
                    setChatMessages(prev => prev.map(m => m.id === aiMessageId ? { ...m, role: 'model', text: "No content to send.", isLoading: false, isError: true } : m ).filter(msg => msg.id !== aiMessageId || msg.isError));
                    setIsChatLoading(false);
                    setAvatarHeaderState('idle');
                    if (searchWasActiveForLastUserTurnRef.current) { 
                        setIsSearchEnabled(false);
                        searchWasActiveForLastUserTurnRef.current = false; 
                    }
                }
            }
        }, [chatMessages, isSearchEnabled, isImageGenEnabled]); 

        const handleRegenerateLastTextMessage = useCallback(async (aiMessageIdToRegenerate) => { 
            isStoppingGenerationRef.current = false; 
            setChatError(null);
            if (!API_KEY || API_KEY === "YOUR_GEMINI_API_KEY") {
                setChatError("API Key is not configured for regeneration.");
                return;
            }
            const originalAiMsgIndex = chatMessages.findIndex(m => m.id === aiMessageIdToRegenerate);
            if (originalAiMsgIndex <= 0 || chatMessages[originalAiMsgIndex-1].role !== 'user') {
                setChatError("Cannot regenerate: Preceding user prompt not found.");
                return;
            }
            
            const userPromptMsg = chatMessages[originalAiMsgIndex-1];
            if (userPromptMsg.attachedFiles?.some(f => f.type?.startsWith('image/'))) {
                setChatError("Regeneration for prompts with images is not yet available for this button.");
                return;
            }
            const aiMsgToRegen = chatMessages[originalAiMsgIndex];
            if (aiMsgToRegen.generatedImageB64 || aiMsgToRegen.imageGenerationError || aiMsgToRegen.isLoadingImage) {
                setChatError("This message was an image. Use the regenerate button on the image itself.");
                return;
            }


            searchWasActiveForLastUserTurnRef.current = isSearchEnabled; 
            imageGenWasActiveForLastUserTurnRef.current = false; 
            
            const shouldUseSearchForThisRegenAI = isSearchEnabled; 
            
            const promptPartsForRegen = [];
            if (userPromptMsg.text) {
                promptPartsForRegen.push({ text: userPromptMsg.text });
            }
            userPromptMsg.attachedFiles?.forEach(file => { 
                 if ((file.name.toLowerCase().endsWith('.txt') || file.type === 'text/plain') && file.apiData?.textContent) {
                    promptPartsForRegen.push({ text: `\n[Start of Text File: ${file.name}]\n${file.apiData.textContent}\n[End of Text File: ${file.name}]\n` });
                }
            });
            const originalUserMessageForRetry = { role: 'user', parts: promptPartsForRegen };

            const newAiPlaceholderId = Date.now().toString() + '_model_regen';
            const aiPlaceholder = { 
                id: newAiPlaceholderId, 
                role: 'model', 
                text:'', 
                isLoading:true, 
                timestamp:new Date(), 
                isError: false, 
                citationMetadata: null, 
                searchUsed: shouldUseSearchForThisRegenAI,
                isLoadingImage: false
            };
            
            const chatHistoryForUiUpdate = chatMessages.slice(0, originalAiMsgIndex);
            setChatMessages([...chatHistoryForUiUpdate, aiPlaceholder]);
            
            if (promptPartsForRegen.length > 0) {
                const historyBeforeUserPromptInternal = chatMessages.slice(0, originalAiMsgIndex - 1);
                const historyForGeminiInit = getGeminiHistory(historyBeforeUserPromptInternal);
                await _streamAiResponse(
                    promptPartsForRegen, 
                    newAiPlaceholderId, 
                    historyForGeminiInit, 
                    shouldUseSearchForThisRegenAI, 
                    false, 
                    originalUserMessageForRetry
                );
            } else { 
                setChatMessages(prev => prev.map(m => m.id === newAiPlaceholderId ? {...m, isLoading: false, text: "No content found to regenerate.", isError: true} : m));
                setAvatarHeaderState('idle');
                 if (searchWasActiveForLastUserTurnRef.current) {
                    setIsSearchEnabled(false);
                    searchWasActiveForLastUserTurnRef.current = false; 
                }
            }
        }, [chatMessages, isSearchEnabled]); 

        const handleRegenerateImage = useCallback(async (failedAiMessageId) => {
             isStoppingGenerationRef.current = false;
             setChatError(null);
             if (!API_KEY || API_KEY === "YOUR_GEMINI_API_KEY") {
                 setChatError("API Key is not configured for image regeneration.");
                 return;
             }

             const failedMsgIndex = chatMessages.findIndex(m => m.id === failedAiMessageId && m.role === 'model');
             if (failedMsgIndex === -1) {
                 setChatError("Could not find the message to regenerate.");
                 return;
             }
             const failedMsg = chatMessages[failedMsgIndex];

             if (!failedMsg.userImageRequestPrompt) {
                 setChatError("Could not find the original prompt for image regeneration.");
                 return;
             }
             
             imageGenWasActiveForLastUserTurnRef.current = true; 
             searchWasActiveForLastUserTurnRef.current = false;

             const newAiImagePlaceholderId = Date.now().toString() + '_model_image_regen';
             const newPlaceholder = {
                id: newAiImagePlaceholderId,
                role: 'model',
                timestamp: new Date(),
                isLoading: false,
                isLoadingImage: true,
                imageGenerationError: null,
                generatedImageB64: null,
                text: '',
                userImageRequestPrompt: failedMsg.userImageRequestPrompt
             };
            
             const messagesUpToFailed = chatMessages.slice(0, failedMsgIndex);
             setChatMessages([...messagesUpToFailed, newPlaceholder]);
             
             await triggerImageGenerationProcess(failedMsg.userImageRequestPrompt, newAiImagePlaceholderId);
        }, [chatMessages]);


        return (
            <div className="h-full text-text-primary flex flex-col font-sans items-center justify-start sm:justify-center p-0">
                <ChatInterface 
                    messages={chatMessages} 
                    onSendMessage={handleSendChatMessage} 
                    isLoading={isChatLoading} 
                    onStopGeneration={handleStopGeneration} 
                    onRegenerateLastTextMessage={handleRegenerateLastTextMessage} 
                    onClearChat={handleClearChat} 
                    setChatError={setChatError} 
                    avatarHeaderState={avatarHeaderState} 
                    isSearchEnabled={isSearchEnabled} 
                    onToggleSearch={handleToggleSearch}
                    isImageGenEnabled={isImageGenEnabled}
                    onToggleImageGen={handleToggleImageGen}
                    onRegenerateImage={handleRegenerateImage}
                    onOpenImagePreview={handleOpenImagePreview}
                />
                {previewImage.src && (
                    <ImagePreviewModal
                        src={previewImage.src}
                        altText={previewImage.alt}
                        onClose={handleCloseImagePreview}
                    />
                )}
            </div>
        );
      };

      const root = ReactDOM.createRoot(document.getElementById('root'));
      root.render(<React.StrictMode><App /></React.StrictMode>);
    </script>
  </body>
</html>
